\documentclass[12pt]{article}
\usepackage[margin = 1 in]{geometry}
\usepackage{amsmath, amsfonts, graphicx, amsthm, quiver}

\newcommand{\F}{\mathbf{F}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\GL}{\mathrm{GL}}
\newcommand{\A}{\mathbb{A}}
\newcommand{\Aff}{\mathrm{Aff}}
\renewcommand{\vec}{\overrightarrow}
\newcommand{\PGL}{\mathrm{PGL}}
\newcommand{\Res}{\mathrm{Res}}
\newcommand{\Z}{\mathbb{Z}}
\renewcommand{\S}{\mathbb{S}}

\newtheorem{fact}{Fact}
\newtheorem{definition}{Definition}
\newtheorem{example}[]{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}

\title{Algebraic Geometry Notes}
\author{Raman Aliakseyeu}
\date{Winter 2024}

\begin{document}
    \maketitle
    \noindent \textbf{Course by Daniil Rudenko.} \par 
    We will start with projective geometry, and then add algebra to it. No schemes in this course, we will be closer to the 19th century stuff. We are not following any particular book(s). Number one book is "Algebraic Geometry" by Shafarevich, our goal is all of chapter 1. We will start with projective geometry, a book for that is Prasolov (roughly). 
    \section{Projective Geometry} 
    \subsection{Lecture 1} 
        Algebraic geometry studies solutions to systems of algebraic equations/algebraic curves (which are loci of solutions of algebraic equations). One of the most famous renaissance geometry results is the Pascal theorem: the points $G, H, I$ in the diagram below are colinear for any configuration of $A, B, C, D, E, F$ on the circle. 
        \begin{center}
            \includegraphics[width = 0.5 \linewidth]{pascals-theorem.PNG}
        \end{center}
        A similar result to that is Pappus' theorem. Another top theorem is by Cayley-Salman 1849: A smooth projective cubic surface contains exactly 27 lines. Another such theorem is that any smooth projective curve of degree 4 has 28 bitangent lines (tangent in exactly two points). Working toward proofs of these results using algebraic geometry is our goal. \par 
        \textit{History of the subject}: throughout the 19th century it developed very quickly, people who were into Euclidean geometry just transitioned to algebraic geometry and discover miracles like we described above. During the late 19th century Italians, who before led algebraic geometry, started producing very incorrect results because the technical aspects became too hard. Oscar Zariski and others saved the subject by rigorizing it using commutative algebra, with the language of ideals and rings, etc. We will see some `wrong' arguments by the Italian school and see why they are not precise enough. There was another revolution by Grothendieck and the Bourbaki group in developing the language of schemes, which we will not touch in this course (``You will need to sacrifice a year of your life to start speaking that language. I never needed to, but a lot of people do.'')\par 
        (Rudenko plug: woollymathematics.com)\par 
        Take $\F$ to be some field (usually $\C$), and polynomials $p_1, \dots, p_k \in \F[x_1, \dots, x_n]$, and study the solutions to the system 
        $$\begin{cases}
            p_1(x_1, \dots, x_n) = 0 \\
            \vdots \\
            p_k(x_1, \dots, x_n) = 0
        \end{cases}$$
        \begin{fact}
            If $k = 1$ and $p_1(x) = x^d + a_{d-1}x^{d-1} + \dots + a_0$ for $d \geq 1$, $p_1(x) = 0$ has $\leq d$ solutions in $\F$. 
        \end{fact}
        If $k = 2$, the number of solutions is $\leq$ than the product of the degrees of the polynomials. Of course we usually want a precise number of solutions, but generally we can't hope for anything more than upper bounds. 
        \begin{fact}
            If $\F = \C$, then $p(x) = 0$ has a solution. 
        \end{fact}
        This is a remarkably hard result called the Fundamental Theorem of Algebra. There is a result that is in a way easier:
        \begin{fact}
            If $\F = \C$, then $p(x) = 0$ has exactly $d$ solutions if counted with multiplicity. 
        \end{fact} 
        We would hope for something like that but for $k > 1$. Bezout's theorem kind of satisfies that, but the picture is more complicated. For example, take a line and a hyperbola. While we would expect $4$ solutions up to multiplicity, but take a line intersecting one of the components of the hyperbola transversally that is parallel to one of the other component's asymptotes. However, everything becomes nicer if we change the complex plane to the projective plane. The study of geometry on the projective plane is the projective geometry, and that's what we will study first. \par 
        \begin{definition} \label{def:proj_plane}
            Let $\F$ be a field. A \textbf{projective space} $\P_\F^n$ is the set of lines in the vector space $\F^{n+1}$. 
        \end{definition}
        An example for $\F = \R$, $\P_{\R}^2$ is the set of lines through the origin in the real plane. We can think of $\P_\R^1$ as all the points of a line in $\R^2$ not through the origin (say the line $y = 1$ in $\R^2$) but with a point at infinity added. \par 
        In $\R^2$, usually two lines intersect at one point, and two points are contained in one line. We can also define $\P_\R^2$ as the set of points of $\R^2$, and the set $[l]$ (points at $\infty$), which are equivalence classes of lines in $\R^2$ by $l_1 \sim l_2$ for $l_1$ and $l_2$ are parallel. Note that there is one more line, line at infinity, consisting only of points $[l]$. \par 
        \begin{fact}
            In $\P^2_\R$ every two distinct lines intersect in one point and every two points are contained in precisely one line. 
        \end{fact}
        \begin{proof}
            Non-parallel lines intersect in the usual way. Parallel lines intersect at their equivalence class $[l]$. If one of the lines is the `line at infinity', they intersect at the equivalence class of the other line. Similarly we can examine the second part of the statement. 
        \end{proof}
        \begin{fact}
            For $\P_\R^2$, the abstract definition \ref{def:proj_plane} is equivalent to the definition we gave in the example. 
        \end{fact}
        \begin{proof}
            Take a plane in $\R^3$ not passing through the origin, $z = -1$ for example. Then we can create a bijection between the set of lines through the origin and the set of points on the plane and the set of points at infinity, by mapping the lines that intersect the plane with their intersection point, and those that are parallel with the plane to their equivalence class on the plane $\R^2$. Thus we have a bijection between the two objects we defined. 
            \begin{center}
                \includegraphics[width = 0.4\linewidth]{proj-plane.jpg}
            \end{center}
        \end{proof}
        Another way to think of $\P_\R^2$ is as a sphere with antipodal points identified. \par
        ``ChatGPT can make you Snake 4: Snake but on the projective plane.''
        
    \subsection{Lecture 2} 
    A \textbf{geometry} is a set $X$ along with a group $G$ acting on $X$, $x \mapsto gx$, which defines `equivalences' between subsets of $X$. 
    \begin{example}
        \begin{itemize}
            \item Let $X = \R^2$, $G$ the group of \textbf{isometries}: distance preserving bijections. Indeed, isometries also preserve lengths, areas, angles, etc. Think of them as congruences in Euclidean geometry. 
            \item Let $X$ be a vector space, $G = \text{GL}(V)$, the general linear group. More specifically, let $X = F^n$ for some field $G$, then $G = \GL_n(F)$.  
        \end{itemize}
    \end{example}
    \begin{definition}
        Fix $F$ some field. Define \textbf{affine space} $\A_F^n = F^n$, with the group of \textbf{affine transformations} $$\mathrm{Aff}_n = \{f: \A^n \to \A^n \mid f(x) = Ax + b, A \in \GL_n(F), b \in A^n\}$$
    \end{definition}
    A typical way of thinking about affine space is as a vector space without a fixed origin. 
    \begin{example}
        Circles don't make much sense in $\A_\R^2$ anymore, since $x^2 + y^2 = 1$ is equivalent to an ellipse (say by $(x, y) \mapsto (2x, 5y)$). 
    \end{example}
    \begin{definition}
        An \textbf{affine algebraic variety} is a set of solutions of a system of polynomials $P_1, \dots, P_k \in F[x_1, \dots, x_n]$:
        $$\begin{cases}
            P_1(x_1, \dots, x_n) = 0 \\
            \vdots \\
            P_k(x_1, \dots, x_n) = 0
        \end{cases}$$
    \end{definition}
    \begin{example}
        \begin{itemize}
            \item $P_1(x, y) = 2x + y - 1 = 0$ gives a line; $P_1(x, y) = x^2 + y^2 - 1 = 0$ gives a circle.
            \item $P_1(x, y, z) = 2x+y-z-1 = 0$ and $P_2(x, y, z) = x + y -3 = 0$, then the variety is an intersection of two planes, thus a line.  
        \end{itemize}
    \end{example}
    If $P_1, \dots, P_k$ are linear $\sum a_i x_i + b$ then we will call it \textbf{affine subspace}.\par 
    We can identify pairs of points in $\A^n$ with vectors in a vector space $F^n$, where vectors $\overrightarrow{AB}$ are pairs of points $(A, B)$, identified via the equivalence relation $\overrightarrow{A_1B_1} \sim \overrightarrow{A_2B_2}$ iff $(A_1)_i - (B_1)_i = (A_2)_i - (B_2)_i$. An observation is that if $f: \A^n \to \A^n$ is an affine transformation, then $f(\overrightarrow{AB}) = \overrightarrow{f(A)f(B)}$. \par 
    What we just described is a homomorphism $\mathrm{Aff}_n \to \GL_n$ which maps $f = Ax + b$ to $A \in \GL_n$. The kernel of this map is isomorphic to $\F^n$ as an additive group. \par 
    \begin{definition}
        If $A_1, \dots, A_{n+1} \in \A^n$ are in \textbf{general position} if $\overrightarrow{A_1A_i}$ for $i = 2, 3, \dots n+1$ are all linearly independent. Equivalently (exercise) $A_1, \dots, A_{n+1}$ are not in the same affine hyperplane. 
    \end{definition}
    \begin{theorem}
        Suppose $A_1, \dots, A_{n+1} \in \A^n$ and $B_1, \dots, B_n \in \A^n$ are points in general position in $\A^n$. Then there is a unique $f \in \Aff_n$ s.t. $f(A_i) = B_i$ for each $i$.  
    \end{theorem}
    \begin{proof}
        [Proof sketch:] Using translation by a vector $\overrightarrow{A_1B_1}$, move $A_1$ to $B_1$. This reduces this problem to the uniqueness of a linear transformation from the basis $\overrightarrow{A_1A_i} \to \overrightarrow{B_1B_i}$. 
    \end{proof}
    Observation: ``$X$ is the midpoint of the segment $\overline{A_1A_2}$'' is an affine property: $f(X)$ is still the midpoint of $\overline{f(A_1)f(A_2)}$. Application of this: 
    \begin{theorem}
        Three medians of a triangle intersect at a point. 
    \end{theorem} 
    \begin{proof}
        Let $ABC$ be an arbitrary triangle in $\A_\R^2$. Let $f$ be the affine transformation taking an equilateral triangle to $ABC$. The medians of an equilateral triangle intersect at a point by symmetry. But medians get sent to medians, and intersections are preserved, so the medians of $ABC$ must intersect also. 
    \end{proof}
    \begin{definition}
        Let $V$ be a vector space over $F$, then its \textbf{projectivization} $\P(V)$ is the set of lines (1-dim vector subspaces) in $V$. 
    \end{definition}
    By definition the dimension of $\P(V)$ is $\dim V - 1$. If $V = F^{n+1}$, $\P(V) = \P_F^n$. More explicitly, $\P(V) = (V - \{0\})/(v \sim \lambda v, \lambda \in F)$. 
    We express points in $\P(V)$ by \textbf{homogeneous coordinates} so $[X_0: X_1 : \dots : X_n]$, with the aforemetioned equivalence relation (so e.g. $[0, 1, 0] = [0, 2, 0]$) \par 
    ``Gold to silver to whiskey, the ratio should be 1 to 1 to 100. You're making a cocktail. Don't do that by the way... Point of projective geometry is to make cocktails.''\par 
    If $A = [X_0: \dots :X_n] \in \P^n$. If $X_n \neq 0$ then $A = [X_0/X_n: \dots : X_{n-1}/X_n, 1]$, points of this form this are in bijection with $\A^n$. Indeed, $\P^n = \A^n \sqcup \A^{n-1} \sqcup \dots \sqcup \A^0$. The points with $X_n = 0$ are in bijection with points in $\P^n$ (divide all entrees by the first non-zero entry from the right). For example, $\P_\R^1 = \R \cup \infty$, where $\R$ is in bijection with points $[X_0/X_1:1]$ and $\infty$ represents $[1:0]$.  \par 
    \textbf{Barycentric coordinates} are basically the same thing as homogeneous coordinates but in a different language. Imagine putting masses $m_1, m_2, m_3$ at the vertices of a triangle, then look at their center of mass. If $m_1 = m_2 = m_3$ then this point is the centroid. More precisely, if $O$ is any point, we have that the point $M$ with homogeneous coordinates $(m_1, m_2, m_3)$, the center of mass for some $m_1, m_2, m_3$ (sum non-zero), is such that
    $$\vec{OM} = \frac{m_1\vec{OA_1} + m_2\vec{OA_2} + m_3\vec{OA_3}}{m_1 + m_2 + m_3}$$
    As such, $O$ has homogeneous coordinates $(m_1:m_2:m_3)$ in the projective plane.  \par 
    A homogeneous polynomial is one where each term has the same degree. Then it makes sense to talk about $P([x_0, \dots, x_d]) = 0$ for a homogeneous polynomial $P$, since such polynomials respect the equivalence relation of homogeneous coordinates. 
    \begin{definition}
        \textbf{Projective variety} is a subset of $\P_F^n$ which is the set of solutions of a system:
        $$\begin{cases}
            P_1([X_0: \dots : X_n]) = 0 \\ 
            \vdots \\
            P_k([X_0: \dots : X_n]) = 0
        \end{cases}$$
        where each $P_i$ is homogeneous. 
    \end{definition}
    \begin{example}
        Say $P(x, y, z) y - x^2$, which we can think of as points $[x:y:1]$ that satisfy the parabola. We can relabel $[x:y:1]$ into $[X:Y:Z]$ with $x = X/Z, y = Y/Z$, so the set of points satisfying $P_1([X:Y:Z])$ is the set such that $X^2 = YZ$. So for instance $[0:1:0]$ is also a solution. We can think of this as the parabola becoming an ellipse because it closes at infinity. 
    \end{example}

    \subsection{Lecture 3}
    \textbf{Observation}: If we have an injective linear map $f: V_1 \to V_2$, and $L$ is a line in $V_1$, then $f(L)$ is also a line. So, $f$ induces a map $\overline{f}: \P(V_1) \to \P(V_2)$.
    \begin{definition}
        If $V_1 = V_2$ in the above, then the set of maps $\P(V) \to \P(V)$ induced by linear maps $V \to V$ form the \textbf{projective linear group} $\PGL(V)$. If $V = \F^n$, we denote $\PGL(V) = \PGL_n(\F)$. 
    \end{definition}
    $\PGL_n(F)$ has a simple description: Let $\Phi: \GL_n(\F) \to \PGL_n(\F)$ be defined by $f \to \overline{f}$. Then $\ker \Phi$ is the set of all diagonal matrices (since those are identity on one-dim spaces), which is isomorphic to $F^\times$. So, $\PGL_n(\F) = \GL_n/F^\times$ by first isomorphism theorem. \par
    Now consider $\P_\F^1$, and $\begin{bmatrix}
        a & b \\ c & d
    \end{bmatrix} \in \GL_n(\F)$. Then $\overline{f}([x: y]) = [ax+by: cx+dy]$. Note that we can interpret $\P_\F^1 = \A_\F^1 \cup \infty$, where $A_\F^1$ can be thought of as the set of all $[x:1]$, and $\infty$ can be thought of as $[0:1]$. So on $\A_\F^1$, $[ax+by: cx+dy] = [\frac{ax+b}{cx+d}:1]$. So on the affine line, $\overline{f}$ sends $x$ to $\frac{ax+b}{cx+d}$. \par 
    If $f(x) = 1/x$, then $[x:y] = [x/y:1]$ gets sent to $[y/x:1] = [y:x]$ by $\overline{f}$. Hence, $[0:1] \mapsto [1:0]$, or in other words $0 \mapsto \infty$ in $\P_\F^1$. This corresponds to the analytic intuition: as $x$ gets closer to $0$, $1/x$ gets closer to infinity. \par 
    Now look at $\P_\F^2$, and consider the linear transformation 
    $$A = \begin{bmatrix}
        a_{11} & a_{12} & a_{13} \\
        a_{21} & a_{22} & a_{23} \\
        a_{31} & a_{32} & a_{33}
    \end{bmatrix}$$
    $\overline{A}$ sends $[x_1: x_2: x_3]$ to $[a_{11}x_1 + a_{12}x_2 + a_{13}x_3: \dots]$. So, as a map on $\A_F^2$, 
    $$\overline{A}(u, v) =  \left( \frac{a_{11}x_1 + a_{12}x_2 + a_{13}x_3}{a_{31}x_1 + a_{32}x_2 + a_{33}x_3}, \frac{a_{11}x_1 + a_{12}x_2 + a_{13}x_3}{a_{31}x_1 + a_{32}x_2 + a_{33}x_3}\right)$$
    ``If you have a dictator you don't like and you want to get rid of them, you can just take a projective transformation of them and send them to infinity. That's what they used to do with dictators.'' \par
    By this he means, by choosing the last row of $A$ appropriately, we can get $\overline{A}$ to send any given line in $\A_\F^2$ to infinity. 
    \begin{theorem}
        Consider points $A_1, \dots A_{n+2}$, $B_1, \dots, B_{n+2}$ in $\P^n$ such that $\{A_i\}$ and $\{B_i\}$ are in \textit{general position} (on $\P^n$ this means for any $i$, $A_1, \dots, \hat{A_i}, \dots, A_{n+2}$ span $F^{n+1}$ when considered as vectors in $F^{n+1}$). Then there exists $\overline{f} \in \PGL_n(\F)$ such that $\overline{f}(A_j) = B_j$ for $1 \leq j \leq n+2$. 
    \end{theorem}
    \begin{proof}
        Let $A_i = [v_i]$, $B_i = [w_i]$, where $[v_i]$ is the equivalence class of some vector $v_i \in \F^{n+1}$. Then by assumption $v_1, \dots, v_{n+1}$ and $w_1, \dots, w_{n+1}$ are bases of $F^{n+1}$, so there exists a unique linear transformation $f_1$ such that $f_1(v_i) = w_i$. Then, $f_1(v_{n+2}) = \tilde{v}_{n+2}$. Since $v_{n+2} = \lambda_1v_1 + \dots + \lambda_{n+1}v_{n+1}$, this maps to $\tilde{v}_{n+2} = \lambda_1w_1 + \dots \lambda_{n+1}w_{n+1}$. Also $w_{n+2} = \mu_1w_1 + \dots + \mu_{n+1}w_{n+1}$. Note that this means $\lambda_i, \mu_i \neq 0$ for any $i$. Then, let $f_2$ be a linear map sending $w_i$ to $\frac{\mu_i}{\lambda_i}w_i$ so $f_2 \circ f_1(v_1) = \frac{\mu_1}{\lambda_i}w_i$ and $f_2 \circ f_1(v_{n+2}) = w_{n+2}$. As such, $\overline{f_2f_1}$ sends $A_i$ to $B_i$. Uniqueness is left as an exercise. 
    \end{proof}
    \begin{theorem}
        [Pappus' Theorem] Consider the diagram 
        \begin{center}
            \includegraphics[width = 0.7\linewidth]{pappus.png}
        \end{center} 
        The points $X, Y, Z$ are always colinear. [Note: the diagram he drew in class had points labelled differently, I changed them because this diagram is from the internet. ]
    \end{theorem}
    \begin{proof}
        Take the line through $Y$ and $Z$, send it via a projective transformation to the line at infinity. Then, to show $X$ is on the same line as $Y, Z$ it suffices to show that after the transformation $\overline{AB'}$ is parallel to $\overline{A'B}$ (marked red in the diagram below:)
        \begin{center}
            \includegraphics[width = 0.8\linewidth]{pappus_step_2.png}
        \end{center} 
        This is true by angle chasing using the parallel line postulate.  
    \end{proof}

    For $\P^1$, recall that $\PGL_2(\F) = \{\frac{ax+b}{cx+d} \mid ad - bc \neq 0\}$. By the theorem above, elements of $\PGL_2(\F)$ send any three distinct points to any three distinct points. This is not true however for four points, so it makes sense that there would be some sort of invariant associated with four points. 
    \begin{definition}
        If $x_1, x_2, x_3, x_4 \in \P^1$ are distinct, \textbf{cross ratio} of these points is defined 
        $$[x_1, x_2, x_3, x_4] = \frac{(x_1-x_2)(x_3-x_4)}{(x_1-x_4)(x_3-x_2)}$$
        If $x_4 = \infty$, 
        $$[x_1, x_2, x_3, \infty] = \frac{x_1 - x_2}{x_3 - x_2}$$
        Similarly define for any other $x_i = \infty$. 
    \end{definition}
    \begin{theorem}
        If $f \in \PGL_2(\F)$ then $[f(x_1), f(x_2), f(x_3), f(x_4)] = [x_1, x_2, x_3, x_4]$. So, cross ratio is an invariant of $\PGL_2(\F)$.
    \end{theorem}
    \begin{proof}
        $\PGL_2(\F)$ as a group is generated by the following transformations: $x \mapsto x + b$, $x \mapsto ax$, $x \mapsto 1/x$. It is easy to see that the first two leave cross ratio unchanged. The third is an algebra bash. 
    \end{proof}
    Note that $[x_1, x_2, x_3, x_4] = [x_{\sigma(1)}, x_{\sigma(2)}, x_{\sigma(3)}, x_{\sigma(4)}]$ for $\sigma \in K$, the Klein $4$-group in $S_4$, so in fact $\sigma \in S_4$ can only change $[x_1, x_2, x_3, x_4]$ into $\lambda, 1-\lambda, 1/\lambda, 1-1/\lambda, \lambda/(\lambda-1), 1/(1-\lambda)$. Proving this will be an exercise. \par 
    \begin{proposition}
        Let $l_1, l_2 \in \P^2$, $O \in \P^2$ not on $l_1$ or $l_2$. Consider a projection of $l_1$ onto $l_2$ about $O$, as in the diagram (with $O$ unlabelled on top): 
        \begin{center}
            \includegraphics[width = 0.4\linewidth]{crossratio.png}
        \end{center}
        More precisely, if $A' \in l_1$ and $\vec{OA'}$ is a line in $\P^2$ containing both $O$ and $A'$, this map is $A' \mapsto A = \vec{OA'} \cap l_2$. This is a projective transformation. 
    \end{proposition} 
    Note that using this diagram, the invariance of cross ratio can be interpreted as the equivalence of ratios segments, as such: 
    $$\frac{|C'A'|}{|C'B'|}: \frac{|D'A'|}{|D'B'|} = \frac{|CA|}{|CB|}: \frac{|DA|}{|DB|}$$
    [A proof of this equality with elementary trigonometry is in Prasolov]
    \begin{proof}
        Let $L_1, L_2$ be planes and $O$ be a line in $V = \F^3$ not contained in either plane. Take the plane $V/O$, then $\P(V/O) = \P^1$. Then $L_i \to V \to V/O$, the composition of inclusion and quotient, gives us a map $\P^1 \to \P^1$. Indeed, we have the commutative diagram
        \[\begin{tikzcd}
            & V && V \\
            {L_1=\mathbb{P}^1} && {V/O=\mathbb{P}^1} && {L_2 = \mathbb{P}^1}
            \arrow[hook, from=2-1, to=1-2]
            \arrow[hook, from=2-5, to=1-4]
            \arrow[from=1-2, to=2-3]
            \arrow[from=1-4, to=2-3]
            \arrow[from=2-1, to=2-3]
            \arrow[from=2-5, to=2-3]
        \end{tikzcd}\]
        With the bottom two arrows being isomorphisms. Composing the left arrow with the inverse of the right arrow gives us a map in $\GL_2(F)$ between $L_1$ and $L_2$, which induces the projection map in $\PGL_2(F)$.  
    \end{proof}

    \subsection{Lecture 4}
    (OH 5PM) \par 
    [TODO: there are some errors in this lecture] \par
    Recall: We consider a field $\F$, mostly $\R, \C$ or $\F_p$. As we defined before, $\P_\F^n = (\F^{n+1} \setminus 0)/\F^\times$, where the identification is $(x_0, \dots, x_n) \sim (\lambda x_0, \dots, \lambda x_n)$. An equivalence class like this is denotes via homogeneous coordinates $[x_0: \dots : x_n]$. We can identify the affine plane $\A_\F^n$ by considering the set of points with $x_n \neq 0$ (i.e. by excluding the lines at infinity). \par 
    Note we may write $R = \F[x_0, \dots, x_n] = R_1 \oplus R_2 \oplus \dots$ where $R_d$ is a vector space of homogeneous polynomials with degree of terms being $d$ (so $x_0^{k_0}x_1^{k_1} \dots x_n^{k_n}$ with $\sum k_i = d$). $R_0$ is the base field. $R_1$ is the dual space $V^*$ of $\F^n$ (since all homog. polys of deg 1 are precisely all linear maps). $R_2 = \mathbb{S}^2 V^*$, and in general $R_d = \mathbb{S}^d V^*$. 
    \begin{definition}
        Let $F \in R_d$ be a homog. polynomial with $d \neq 0$. Then the set of solutions of an equation $F(x_0, \dots, x_n) = 0$ is called a \textbf{hypersurface} of degree $d$ in $\P^n$.  
    \end{definition}
    \begin{example}
        In $\P^3$, $x_0^3 - x_1^3 + x_2x_3^2 + x_3^3 = 0$. The intersection of this with $\A^3$ (obtained by dividing all terms by $x_3$) we get $(x_0/x_3)^3 - (x_1/x_3)^3 + x_2/x_3 + 1 = 0$. The rest of this hypersurface is in $\P^3 \setminus \A^3 = H$, it is defined by $x_0^3 - x_1^3 = 0$. Since $H \cong \P^2$, we can do the same procedure to reduce this hypersurface to something we can draw in $\A^2$, getting $y_0^3 - y_1^3 = 0$, which are three lines intersecting at the origin (if $\F = \C$). Going the other way from $y_0^3 + y_1^3 + y_2 + 1 = 0$ by introducing another variable and going into $\P^3$ is called \textit{homogenization}. 
    \end{example}
    \begin{definition}
        A \textbf{projective variety} is a \textit{finite} intersection of hypersurfaces. 
    \end{definition}
    (Note: you do not get more sets if you extend this to an infinite system, by the Hilbert basis theorem, to be covered later). \par 
    \begin{definition}
        A hypersurface $X = \{F = 0\}$ for $F \in R_d$ is \textbf{irreducible} precisely when $F$ is irreducible. 
    \end{definition}
    \begin{example}
        \begin{itemize}
            \item $x_0x_1 = 0$ in $\P^2$ is a union of two lines. Not irreducible polynomial, so the hypersurface is not irreducible also. In general, any union of two lines is not irreducible. 
            \item $x_0^2 + x_1^2 = x_2^2$ is irreducible, so its hypersurface, a circle in $\A^2$. 
        \end{itemize}
    \end{example}
    Assume $\F = \C$. Consider  the action of $\PGL_{n+1}$ on degree $d$ hypersurfaces in $\P^n$. Take $d = 1$, then this action is transitive, as we saw last time. Thus up to projective transformations all hyperplanes are the same. If $d = 2$, $F$ looks like 
    $$\sum a_{ij} x_i x_j = (x_0, \dots, x_n) A \begin{pmatrix}
        x_0 \\
        \vdots \\
        x_n
    \end{pmatrix} = x^\top A x$$
    Subject to a projective transformation, $x = Uy$ for some $y \in \P^n$. Thus, $x^\top A x \mapsto y^\top U^\top A Uy = 0$.  
    \begin{theorem}
        Any quadric is a projectively equivalent to a quadric $\lambda_0 x_0^2 + \lambda_1 x_1^2 + \dots + \lambda_n x_n^2 = 0$
    \end{theorem}
    [TODO: This is equivalent to (some) matrix diagonalization result, didn't catch which one that was.]
    Exercise: prove this my hand for $n = 2$. \par 
    So, if $\F = \R$, for $n = 2$ the picture is complicated, for example $x_0^2 - x_1^2 - x_2^2 = 0$ is a double cone. But for $\F = \C$ things become simpler because everything can be reduced to the cases $x_0^2 + x_1^2 + \dots + x_i^2 = 0$. So in $\P_\C^2$ we have lines, intersections of two lines, and conics, for $i = 1, 2, 3$ respectively. Thus the action of $\PGL_3$ has three orbits, namely those classes. \par 
    Now we discuss projective duality. Suppose we are working in $\P^2$. A line here is defined by an equation $ax + by + cz = 0$ with not all $a, b, c = 0$, modulo rescaling. Thus lines in $\P^2$ also form a projective space. This has some striking applications. \par
    Conceptually, if we have $\P^n = \P(V)$ for an $n$-dim vector space $V$, hyperplanes in $\P^n$ are in bijection with the elements of $\P(R_1) = \P(V^*)$. \par 
    The \textbf{duality principle} can be stated as follows. Consider any theorem about points and lines in $\P^2$, replacing lines by points and vice versa gives another valid theorem. 
    \begin{example}
        Recall Pappus' theorem. The dual to this theorem can be drawn as follows: (TODO: wrong diagram)
        \begin{center}
            \includegraphics[width = 0.7\linewidth]{pappus dual.png}
        \end{center}
        The statement is that lines $a$, $b$, and $c$ (passing through $C_i$) all pass through $O$. 
    \end{example} 
    In a dualization of a theorem, for every line there is a point, for every point there is a line. If $n$ lines intersect at a point, in the dual theorem $n$ points will lie on one line. \par 
    In $\P^3$ a similar duality holds: points correspond to planes, lines correspond to lines, planes correspond to points. \par 
    Three important facts about $V^*$: 1. $\dim V = \dim V^*$ since $V$ and $V^*$ are (not canonically) isomorphic, 2. $V \cong (V^*)^*$ canonically via an isomorphism $v \mapsto (\phi: V \to F \mapsto \phi(v))$, 3. There is an inclusion reversing bijection between subspaces of $V$ and $V^*$, which is equivalent to $\dim W + \dim W^\top = \dim V$. 

    \subsection{Lecture 5}
    Let's talk about conics a little bit more. If $\P_\F^n = \P(\F^{n+1})$, so $[x_0: x_1: \dots : x_n]$ are homogeneous coordinates on $\P_\F^n$, and $P(x_0, \dots, x_n)$ is a homogeneous polynomial of degree $d$, $\{[x_0, \dots, x_n] \mid P(x_0, \dots, x_{n+1}) = 0\} \subset \P^n$ is a hypersurface of degree $d$. For $d = 1$ this is the familiar hyperplane. If $d = 2$ this is a \textit{quadric}. \par 
    Over $\C$ (polynomials with coeffs in $\C$), every quadric is either a union of two lines, a cross of two lines, or a conic (which are all the same on the projective plane). The conic is called the \textit{smooth} quadric. \par 
    \begin{theorem}[Pascal]
        \hfill
        \begin{center}
            \includegraphics[width = 0.4\linewidth]{pascal.png}
        \end{center}
        For any ellipse, putting distinct $A, B, C, D, E, F$ on it and connecting them a la Pappus' theorem gives us colinear intersection points $X$, $Y$, $Z$. 
    \end{theorem}
    ``Who thinks this is beautiful? If you don't [points to door]''\par 
    In this way Pappus' theorem is not surprising because both a pair of lines and a conic are quadrics over $\C$. \par 
    \begin{lemma}[Chasles]
        Let points $A_1, A_2, A_3, A_4$ and $X$, $Y$ lie on a conic. Then 
        $$[(XA_1), (XA_2), (XA_3), (XA_4)] =  [(YA_1), (YA_2), (YA_3), (YA_4)]$$
    \end{lemma}
    Notice that there is a bijection between points of a conic and lines through some particular point $X$ on a conic $C$. Indeed, a line through $X$ is either tangent to the conic or intersects it at some other point. Associating the tangent line to $X$ and the other lines to their intersection points, we associate the points of $C$ with $\P^1$. Indeed, the result above follows by using the two bijections onto $\P_1$ given by $X$ and $Y$, and concluding that there is a projective transformation mapping the $X$ configuration to the $Y$ configuration. This however requires some details we haven't gone over, so below is an elementary proof:
    \begin{proof}
        (TODO: diagram) First, assume the conic $C$ is a circle in $\R^2 \subset \P_\R^2$. If the points are ordered $A_1, \dots, A_4$ from left to right, let $U$ and $V$ be intersection points of a line $A_1A_4$ through $XA_2, XA_3$. Then
        $$[XA_1, \dots, XA_4] := [X_1, U, V, X_4] = \pm \frac{S_{A_1XY} S_{VXA_1}}{S_{A_1XA_2}S_{UXV}} = \frac{\sin \angle X_1XU \sin \angle VXA_1}{\sin \angle A_1XA_2 \sin \angle UXV}$$
        where $S_{ABC}$ is the notation for the area of triangle $ABC$. But by [these angles from $X$ and $Y$ are called something I forget what], we know that they remain invariant if we replace $X$ with $Y$. We thus have shown that the cross ratio remains the same. 
    \end{proof}
    \begin{proof}
        [Proof: (Of Pascal's)] By the lemma above, 
        $$[A_1B_1, A_1B_2, A_1B_3, A_1A_2] = [A_3B_1, A_3B_2, A_3B_3, A_3A_2]$$
        Now label the two intersection points above the line $XYZ$ in the diagram $S$ and $T$. Then by definition of cross ratio of four lines, 
        $$[B, Z, S, A_2] = [T, X, B_3, A_2]$$
        by picking the transversal line to be $A_2B_1$ and $A_2B_3$ for the left and right cross ratios respectively. But this quantity is also equal to $[(B_1Y), (ZY), (SY), (A_2Y)]$. We want $X$ and $Z$ to be colinear with $Y$, let $X'$ be the intersection of the line $ZY$ with $A_2B_3$. Then, 
        $$[(B_1Y), (ZY), (SY), (A_2Y)] = [T, X', B_3, A_2] = [T, X, B_3, A_2]$$
        This implies $X = X'$, which concludes the proof. 
    \end{proof}
    Our goal now is to go towards Bezout's theorem. The statement of the theorem is as follows: In $\P_\C^2$ let $X$ and $Y$ be projective curves such that they do not have common irreducible components. Then the number of points in their intersection is less than or equal to the product of their degrees, and in fact (hard part) is \textit{equal} up to multiplicity. Of course, it's hard because it's hard to define exactly what `up to multiplicity' means. Take two concentric circles $x^2 + y^2 = 1$ and $x^2 + y^2 = 2$ in $\A^2$. These only have solutions once you homogenize them and pass over to the complex projective plane. Then, $x^2 + y^2 = z^2, x^2 + y^2 + 2z^2$ have solutions $[1:i:0], [1:-i:0]$. By Bezout's these points have to have intersection multiplicty $2$, so in fact they are \textit{tangent points}. \par 
    For now, we will only prove the easier upper bound on the number of intersection points. 
    \begin{theorem}[Sylvester]
        Let $A(x) = a_mx^m + a_{m-1}x^{m-1} + \dots a_0 = 0$ and $B(x)$ be polynomials of degree $m$ and $n$ respectively. They have a common factor of degree $\geq 1$ if and only if the following $(m+n) \times (m+n)$ matrix
        $$\begin{bmatrix}
            a_m & a_{m-1} & \dots & a_0 & 0 & \dots & 0 \\
            0 & a_m & \dots & a_1 & a_0 & \dots & 0 \\
            & & \vdots & & \\
            0 & 0 & \dots & a_m & \dots & & a_0 \\
            b_n & b_{n-1} & \dots & &  b_0 & \dots & 0 \\
            0 & b_n & \dots & & & \dots & 0 \\
            & & \vdots & & \\
            0 & 0 & \dots b_n & & \dots & & b_0 \\
        \end{bmatrix}$$
        (where $a_i$ appear in the first $n$ rows, $b_j$ appear in the last $m$ rows) has determinant $0$. The determinant is called the \textbf{resultant} of $A$ and $B$. 
    \end{theorem}
    \begin{proof}
        $A$ and $B$ have a common factor iff there exist non-zero polynomials $U$ and $V$ of degrees less than $n$ and $m$ respectively, such that $A\cdot U = B \cdot V$. This is a linear combination in the first $n$ rows being equal to a linear combination of the last $m$ rows, hence this condition holds iff $\det = 0$.  
    \end{proof}
    You can use this theorem to solve systems of polynomial equations by eliminating a variable, writing the condition that there is a solution in terms of the resultant. This is highly impractical if the degrees of equations are large, but is a useful proof technique. 

    \subsection{Lecture 6}
    Note that to generalize Sylvester's theorem to also work for polynomials over a ring and not a field we need to use Gauss' lemma. 
    \begin{theorem}
        Suppose that $f(x) = a_m(x-t_1)\dots (x-t_m)$, and $g(x) = b_n(x-s_1)\dots (x-s_n)$. Then their resultant is
        $$\Res(f, g) = a_m^n b_n^m \prod_{i=1}^m \prod_{j=1}^n (t_i - s_j)$$
    \end{theorem}
    \begin{proof}
        Let's view $\Res(f, g)$ as a polynomial in $\Z[a_m, b_n, t_1, \dots, t_m, s_1, \dots, s_n]$ (can do by Vieta's formulas). Looking at the resultant matrix, we can extract an $a_m$ and $b_n$ from every row of the determinant, we get $\Res = a_m^n b_n^m P(t_1, \dots, t_m, s_1, \dots, s_n)$. \par 
        Now we mention a little trick: for $f(x)$ with coefficients in a unique factorization domain, $f(x_0) = 0$ iff $(x-x_0)$ divides $f(x)$. Indeed, take $\tilde{f}(x) = f(x + x_0) = \tilde{a}_nx^n + \dots + \tilde{a}_0$. But then $f(x) = \tilde{a}_n(x-x_0)^n + \dots + \tilde{a}_0$, which $(x - x_0)$ divides if and only if $\tilde{a}_0 = 0$, i.e. $x_0$ is a root. \par 
        Thus, $P$ is divisible by $(t_i - s_j)$, since considering $P$ as a polynomial with coefficeints in $\Z[t_1, \dots, \hat{t}_i, \dots, t_m, s_1, \dots]$. Thus, $P(s_j) = 0$ implies $(t_i - s_j) \mid P$. Note that $(t_i - s_j)$ are pairwise coprime, as such 
        $$\Res(f, g) = Ca_m^n b_n^m \prod \prod (t_i - s_j)$$
        Note that the product $\prod \prod (t_i - s_j)$ has the same degree $nm$ as $\Res(f, g)$ (exercise), so by plugging in a point we realize $C = 1$. 
    \end{proof}
    (Discriminants, Resultants, and Multidimensional Determinants - very underrated book)\par 
    \begin{theorem}
        [Weak Bezout] Let $F$ be an infinite field, consider two curves with no common irreducible components in $\P_\F^2$, defined by $P(X, Y, Z) = 0$, $Q(X, Y, Z) = 0$ with $P$ and $Q$ homogeneous (no common irreducible components means $P$ and $Q$ are coprime). Then, $P$ and $Q$ have at most $\deg P \deg Q$ solutions. 
    \end{theorem}
    \begin{proof}
        Let $\deg P = m$, $\deg Q = n$. Assume the system $\{P = 0, Q = 0\}$ has $nm + 1$ solutions of the form $(X_i, Y_i, Z_i)$. Note there exists a coordinate system such that \textbf{1}. $P$ and $Q$ are not divisible by $Z$ and \textbf{2}. $Z_i \neq 0$ \textbf{3}. $\frac{Y_1}{Z_1}, \dots, \frac{Y_{n+m}}{Z_{n+m}}$. Then we have $f(x, y) = \frac{P(X, Y, Z)}{Z^m}$, $g(x, y) = \frac{Q(X, Y, Z)}{Z^n}$ are polynomials of degree $m$ and $n$ respectively in $X/Z$, $Y/Z$. They also have $nm + 1$ solutions $(x_i, y_i)$ with all $y_i$ distinct, and $f$ and $g$ have no common factor of degree $\geq 1$. Let's view $f(x, y), g(x, y)$ as polynomials in $(\F[y])[x]$. Note that $\Res(f, g)$, a polynomial in $\F[y]$, is nonzero with roots $y_i, \dots, y_{nm+1}$. It remains to show that $\deg \Res(f, g) < nm$. Left as a (non-trivial) exercise. 
    \end{proof}
    An application of this theorem. Suppose we are working in $\P_\C^2 = \P(\C^3)$. Curves of degree $1$ are lines, they form $\P(V^*) = \P^2$ as we have seen before. Curves of degree $2$ of form $ax^2 + bxy + cy^2 + dxz + eyz + fz^2$ form a space $\P^5$ (one less than the coordinates). All degree $2$ curves containing a point $[x_0, y_0, z_0]$ form a ($4$-dimensional) hyperplane in $\P^5$. Space of degree $2$ curves containing $2$ points $P_1, P_2$ will have dimension at least $3$, but it can be $4$. Degree $2$ curves containing $P_1, P_2, P_3$ will have dimension at least $2$, ... we thus can show that any $5$ points in $\P^2$ lie on a curve of degree $2$. 

    \subsection{Lecture 7}
    Consider $\P^5 = \P(\S^2V^*)$, the space of quadrics $F = a_1x^2 + a_2xy + \dots + a_6$ in $\P^2$, corresponding to the coordinate $[a_1:a_2\dots :a_6]$. For a $P \in \P^2$, the set $H_P$ of quadrics containing $P$ forms a hyperplane in $\P^5$. \par 
    [The proof of this will probably be on the exam]
    \begin{theorem}
        Let $P_1, \dots, P_5 \in \P^2$ be points in general position (no three points lie on the same line). Then there exists a unique quadric containing $P_1, \dots, P_5$. 
    \end{theorem}
    \begin{proof}
        Consider $H_1 \cap \dots \cap H_5$ in the space of quadrics $\P_\C^5$ (of dimension $6$), need to show that this is precisely one point. Since intersecting hyperplanes drops dimensions by at most $1$, we know that $H_{P_1} \cap \dots H_{P_5}$ has dimension at least $1$. Thus there is a quadric containing $P_1, \dots, P_5$. Assume it is not unique, so $C_1$, $C_2$ are two distinct quadrics containing those points, as such $|C_1 \cap C_2| \geq 5$ which is impossible, say taking $C_1$, $C_2$ to be lines. (TODO: confused about dimensions. Are we considering $\P^5$ in $\C^6$?)
    \end{proof}
    [Reproducing this or something like it will be on the midterm]
    \begin{theorem}
        [Pascal's theorem on conic] Take $C = 0$ defining a conic, $A_1, \dots, A_6$, $X = A_1A_5 \cap A_2A_4$, .... Then $X, Y, Z$ lie on the same line. (TODO diagram, $A_1, \dots, A_6$ go clockwise on ellipse)
    \end{theorem}
    \begin{proof}
        Let $L_{UV}$ be the equation for the line through $U, V$. Consider curves $L_{A_6A_2}L_{A_5A_3}L_{A_1A_4}$ and $L{A_1A_5}L_{A_2A_4}L_{A_3A_6}$. These intersect at $A_1, \dots, A_6, X, Y, Z$. For varying $\lambda \in \C$, consider the pencil of cubic curves $L_{A_6A_2}L_{A_5A_3}L_{A_1A_4} + \lambda L{A_1A_5}L_{A_2A_4}L_{A_3A_6}$ in $\P^9$ (projective space of cubic curves). Consider any point $P \in C$ that is not $A_1, \dots, A_6$. Then there exists $\lambda_p \in \C$ such that $L_{A_6A_2}L_{A_5A_3}L_{A_1A_4} + \lambda_p L{A_1A_5}L_{A_2A_4}L_{A_3A_6}$ [Take $\lambda_p = \frac{-L_{A_4A_2}(P)L_{...}(P)L_{...}(P)}{L(...)L(...)L(...)}$]. But then $L_{A_6A_2}L_{A_5A_3}L_{A_1A_4} + \lambda_p L{A_1A_5}L_{A_2A_4}L_{A_3A_6} = CL$ (TODO)
    \end{proof}


\section{Algebraic Geometry}
    For $\A^n$, $R = F[x_1, \dots, x_n]$, and an ideal $I \subset R$ we have a \textbf{vanishing ideal} $V(I) = \{(a_1, \dots, a_n) \in \A^n \mid f(a_1, \dots, a_n) = 0 \text{ for any } f \in I\}$. For finitely generated $I = (f_1,\dots, f_n) = \{g_1f_1 + \dots + g_nf_n\}$, we have $V(I)$ is the set of solutions of $\{f_i(x_1, \dots, x_n) = 0\}$. We will prove soon that all ideals of $F[x_1, \dots, x_n]$ are finitely generated. People name $V(I)$ \textbf{affine sets}, affine algebraic sets, affine algebraic varieties, or affine closed sets. \par 
    Some properties: 
    \begin{enumerate}
        \item $V((1)) = \emptyset$.
        \item $V((0)) = \A^n_\F$.
        \item $V(I_1 \cup I_2) = V(I_1) \cap V(I_2)$ (inclusion $\supset$ is obvious. For $p \in V(I_1 \cup I_2)$, if $p \not \in V(I_1)$, $f \in I_1$, so $f(p) \neq 0$, same with $I_2$, so $fg \in I_1 \cap I_2$... )
        \item $\cup V(I_\alpha) = V(\prod I_\alpha)$, where $\prod I_\alpha$ is the ideal generated by polynomials $f_1f_2\dots $ where $f_1 \in I_1, f_2 \in I_2, \dots$. 
    \end{enumerate}
    These properties imply that affine sets are closed sets of a topology, called the \textbf{Zariski topology}. \par 
    ``I have 20 more minutes? *Claps hands*''\par 
    \begin{example}
        \begin{enumerate}
            \item If $\A_\C^1$, $R = \C[x]$, thus affine algebraic sets are either finite collections of points or $\A^1$.
            \item In $\A^2_\C$ we have a more complicated picture, we could have finite collections of points + quadrics and lines. 
        \end{enumerate}
    \end{example}

    \begin{definition}
        Suppose $R$ is a ring (commutative with $1$). $R$ is \textbf{Noetherian} if (TFAE)
        \begin{enumerate}
            \item Every ideal $I \subset R$ is finitely generated. 
            \item Every ascending chain $I_1 \subset I_2 \subset I_3 \subset \dots \subset R$ stabilizes, so for some $N$, $I_{N} = I_{N+1} = I_{N+2} = \dots$. 
        \end{enumerate} 
    \end{definition}
    Equivalence of these two conditions is on HW3. Geometrically, the Noetherian condition will tell us that infinite systems don't give us any new options for affine sets. \par 
    \begin{example}
        \begin{enumerate}
            \item A field $\F$, every ideal is generated by $0$ or $1$. 
            \item $\Z$, $\F[x]$, generated by $a \in \Z$ and $x^n$ respectively. 
            \item If $R$ is Noetherian, $R/I$ is Noetherian. 
            \item Hilbert basis theorem: If $R$ is Noetherian, $R[x]$ is Noetherian. 
            \item $F[x_1, \dots, x_n]$ is Noetherian. 
            \item By the above $F[x_1, \dots, x_n]/I$ is Noetherian. In particular, we will study $\C[x, y]/(x^2 + y^2 + 1)$, vanishing functions on a circle. 
        \end{enumerate}
    \end{example}
    (Akhil Spectral theory notes are very good)\par 
    \begin{theorem}
        [Hilbert Basis Theorem] $R$ Noetherian if and only if $R[x]$ is Noetherian. 
    \end{theorem}
    \begin{proof}
        Say $I \subset R[x]$ is an ideal, and let $f_1$ be an element in $I$ of the smallest degree, $f_2$ be the element of smallest degree in $I \setminus (f_1)$, $f_3$ smallest degree in $I \setminus (f_1, f_2)$, ... . Then we get a chain of distinct ideals $(f_1) \subset (f_1, f_2) \subset (f_1, f_2, f_3) \subset \dots$. Let's look at the leading coefficients $a_i$ of $f_i$. Then, $(a_1, \dots, ) = (a_1, \dots, a_n)$ is finitely generated. Thus, $a_{n+1} \in (a_1, \dots, a_n)$, and as such $a_{n+1} = a_1\lambda_1 + \dots + a_n\lambda_n$. Suppose we have $f_{n+1} \in (f_1, \dots, f_n)$. Then, let 
        $$g = f_{n+1} - \lambda_1 f_1 x^{\deg f_{n+1} - \deg f_1} - \dots - \lambda_n f_n x^{\deg f_{n+1} - \deg f_n}$$
        so $\deg g < \deg f_{n+1}$, but $g \in I \setminus (f_1, \dots, f_n)$, which contradicts the choice of $f_{n+1}$. 
    \end{proof}

    \subsection{Lecture 8}
    There exists a correspondence between affine algebraic sets in $\A^n$ and the ideals on $k[x_1, \dots, x_n]$. Indeed, we can take any ideal $I$ on $k[x_1, \dots, x_n]$ and consider its vanishing set $V(I)$. Note the following properties we reviewed last time: $V(I \cap J) = V(I) \cup V(J)$ and $I \subset J \implies V(I) \supset V(J)$. This correspondence can be reversed: take some affine algebraic $X$ and consider $I(X) = \{f \in k[x_1, \dots, x_n] \mid \forall p \in X, f(p) = 0\}$. So given $X$, relate to it the ideal of functions that vanish on it. \par 
    Note that this correspondence is not invertible: consider $I \subset \R[x, y]$ generated by $x^2 + y^2 + 1 = 0$. The vanishing set of this is empty, but $I(\emptyset) = \R[x, y]$. Also could consider $k = \C$ and first consider $I = (x^2)$. However 
    \begin{proposition}
        For any affine algebraic set $X \subset \A^n$ we have $V(I(X)) = X$. So, $X \to I(X)$ is an injective map.
    \end{proposition}
    \begin{proof}
        Set $X = V(f_1, \dots, f_n)$, solutions to the system of $f_i$. If $p \in X$, then $p \in V(I(X))$. On the other hand if $p \in V(I(X))$, every $f$ which vanishes on $X$ vanishes on $p$. Thus, all $f_i$ vanish on $p$ so $p \in X$. 
    \end{proof}
    \begin{definition}
        Let $I \subset R$ be an ideal in a ring. The \textbf{radical} of $I$, denoted $\sqrt{I}$, is given by $\{z \in \R \mid \exists N\: z^N \in I\}$. 
    \end{definition}
    This is an ideal (homework). An example of how this works is $\sqrt{(x^2)} = (x)$. 
    \begin{definition}
        $I$ is \textbf{radical} if $\sqrt{I} = I$. 
    \end{definition}
    \begin{theorem}
        [Hilbert's Nullstellensatz] For an ideal $J \subset k[x_1, \dots, x_n]$ ($k$ is algebraically closed), we have $I(V(J)) = \sqrt{J}$.  
    \end{theorem}
    Example: $(x^2) \xrightarrow{V} 0 \in \A^1 \xrightarrow{I} (x) = \sqrt{(x^2)}$. \par 
    `Middle school version': consider $f_1, \dots, f_m \in \C[x_1, \dots, x_n]$. Suppose that $g \in \C[x_1, \dots, x_n]$ vanishes at every solution of the system $\{f_i = 0\}_i$. Then there exists some natural $N$ such that $g^N \in (f_1, \dots, f_n)$, an ideal of $\C[x_1, \dots, x_n]$. \par 
    \begin{definition}
        An affine algebraic set $X$ is called \textbf{irreducible} if it cannot be presented as a union of affine algebraic sets. Thus, if $X = X_1 \cap X_2$ is irreducible, $X_1$ or $X_2$ is $X$. 
    \end{definition}
    \begin{theorem}
        $X$ is irreducible iff $I(X)$ is prime (recall: $I$ prime if $ab \in I$ then either $a$ or $b$ in $I$). 
    \end{theorem}
    \begin{proof}
        Suppose $I(X)$ is not prime, so there exist $f, g \in k[x_1, \dots, x_n]$ so $fg \in I(X)$ but $f, g \not \in I(X)$. Take $X_1 = X \cap V(f)$, $X_2 = X \cap V(g)$ ($V(f)$ is the vanishing set). Then $X = X_1 \cup X_2$, but $X_1, X_2 \neq X$. \par 
        Suppose $X$ is not irreducible, so $X = X_1 \cup X_2$ with $X_1, X_2 \neq X$. So $I(X) \neq I(X_1)$, in fact $I(X) \subset I(X_1)$, similarly $I(X) \subset I(X_2)$ (subsets strict). As such, there exist $f \in I(X_1)$, $g \in I(X_2)$, thus $fg$ vanishes on $X_1 \cup X_2$, so $fg \in I$, and $I$ is not prime. 
    \end{proof}
    \begin{theorem}
        Suppose $X \subset \A^n$ is an affine algebraic set. Then
        \begin{enumerate}
            \item There exist irreducible affine sets $X_1, \dots, X_m$ such that $X = X_1 \cup \dots \cup X_m$, and $X_i \supset X_j$ (strict). 
            \item This decomposition is unique up to relabeling the sets $X_i$. 
        \end{enumerate} 
    \end{theorem}
    \begin{proof}
        If $X$ is irreducible, we are done. Otherwise, $X = X_1 \cup X_2$, a union of proper subsets. Continue recursively until we obtain a binary tree of affine sets, where the leaves (terminating nodes) are irreducible. Suppose for contradiction the tree we obtain this way is infinite, then there exists an infinite path down the tree: $X \supset X^{(1)} \supset X^{(2)} \supset \dots $. Then $I(X) \subset I(X^{(1)}) \subset I(X^{(2)}) \subset \dots $. But this contradicts the fact that $X$ is Noetherian. \par 
        Suppose we have two decompositions $X_1 \cup \dots \cup X_m = X_1' \cup \dots X_m'$. Take any $X_i$, then $X_i = X \cap X_i = (X_1' \cap X_i) \cup (X_2' \cap X_i) \dots \cup (X_m' \cap X_i)$. Because $X_i$ is irreducible, this implies $X_i = X_i \cap X_j'$, and as such $X_i \subset X_j'$. But performing this same argument for $X_j'$ we get $X_i \subset X_j' \subset X_k$, so $X_i = X_k$, and $X_i = X_j'$. 
    \end{proof}
    Main example of affine sets are hypersurfaces: if $f(x_1, \dots, x_n)$ is an irreducible polynomial of non-zero degree, its vanishing set is an irreducible \textbf{hypersurface}. An example would be something like $xyz \pm x^2 + y^2 + z^2 + 1 = 0$. \par 
    Recall our correspondence between algebraic sets and ideals of $k[x_1, \dots, x_n]$ from the beginning. Note that by Nullstellensatz $I(X)$ for any affine algebraic set $X$ is a radical ideal. So, restricting our correspondence to radical ideals, is correspondence becomes bijective. This same correspondence carries irreducible sets to prime ideals, hypersurfaces to principal ideals, and maximal ideals to points. \par 
    Now we need some notions from commutative algebra. \par 
    Suppose $A$ and $B$ are algebras over the same field $k$. Assume $A - B$, so $A \supset B$ as sets. Say that $A$ is \textbf{finitely generated over $B$} if there exist $a_1, \dots, a_n \in A$ such that $A = B[a_1, \dots, a_n]$: every $a \in A$ is a polynomial in $a_1, \dots, a_n$ with coefficients in $B$. Example: $k[x_1,\dots, x_n]$ finitely generated algebra over $k$. Another one is $\C[x, y]/(x^2 + y^2 - 1)$ is a finitely generated algebra over $\C$. \par 
    We say $A$ is \textbf{finite} over $B$ [if $A$ is a finitely generated $B$-module] if there exist $a_1, \dots, a_n \in A$ such that for all $a \in A$ there are $b_1, \dots, b_n \in B$ so $a = a_1b_1 + \dots a_n b_n$. In other words, $A = a_1B + a_2B + \dots + a_nB$. Example: $\C[x]/(x^3 - 1)$, this is a finite algebra over $\C$, in fact a three-dimensional vector space. If your algebra is also a field, another algebra being finite over it means you have a finite-dimensional vector space.  

    \subsection{Lecture 9}
    \begin{definition}
        $A$ if \textbf{finitely generated over $B$} if there exist $a_1, \dots, a_n \in A$ such that every element in $A$ is a polynomial in $a_i$ with $B$-coefficients: $A = B[a_1, \dots, a_n]$. In other words, $B[x_1, \dots, x_n] \to A$ with the map being the evaluation homomorphism $f \mapsto f(a_1, \dots, a_n)$ is surjective. In other words, $A \cong B[x_1, \dots, x_n]/I$. 
    \end{definition}
    Another definition, $A$ is finite over $B$ [$A$ is finite as a $B$-module] if there exist $a_1, \dots, a_n$ if for every $a \in A$ there exist $b_1, \dots, b_n$ such that $a = b_1a_1 + \dots + b_na_n$.\par 
    Here's a key bit from commutative algebra: 
    \begin{theorem}
        Let $k$ be a field of characteristic $0$. Suppose that $A$ is a finitely generated algebra over $k$, so $A = k[x_1, \dots, x_n]/I$. Assume $A$ is a field. Then $A$ is a finite algebraic extension of $k$. 
    \end{theorem}
    (Choosing $k = \C$, we know that every finitely generated algebra over $\C$ that's a field must be $\C$ itself. )\par
    We now want to show that in the correspondence we talked about last lecture, finite sets of points correspond to maximal ideals of $k[x_1, \dots, x_n]$. \par 
    Take $P = (a_1, \dots, a_n) \subset \A^n$ be a set of points in $\A^n$. Consider that the ideal $(x_1 - a_1, \dots, x_n - a_n)$ generated by these $n$ linear elements is then a subset of $I(P)$. Take any $f \in I(P)$, then $f(a_1, \dots, a_n) = 0$ implies $f(a_1 + y_1, \dots, a_n + y_n) = y_1f_1 + \dots + y_nf_n$ for some functions $f_i \in I(P)$. THen $f(x_1, \dots, x_n) - (x_1 - a_1)f_1 + \dots (x_n - a_n)f_n$ which implies $f \in (x_1-a_1, \dots, x_n-a_n)$. \par 
    We further claim $I(P) = (x_1 - a_1, \dots, x_n - a_n)$ is maximal. Consider the surjective map $k[x_1, \dots, k_n] \to k $ given by the evaluation homomorphism $f \mapsto f(a_1, \dots, a_n)$. Its kernel is $I(P)$. Then $k[x_1, \dots, x_n]/(x_1- a_1, \dots, x_n - a_n) \cong k$, which implies $I(P)$ is maximal. \par 
    Now, for every maximal ideal $M \subset k[x_1, \dots, x_n]$ there exist $a_1, \dots, a_n$ such that $M = (x_1 - a_1, \dots, x_n - a_n)$. Indeed, consider $k[x_1, \dots, x_n]/M$, a finitely generated algebra over $k$, so by theorem it is finitely generated over $k$. Then, looking at the map $k \to k[x_1, \dots, x_n]/M$, we know there are $a_1, \dots, a_n$ such that $k$ maps $a_i$ and $x_i$ to the same coset. Hence, $x_i - a_i \in M$, and $(x_1 - a_1, \dots, x_n - a_n) \subset M$. Because both sides are maximal ideals, this in fact implies equality. This proves the correspondence between points and maximal ideals. \par 
    Now we show weak Nullstellensatz: the empty set corresponds to all of $k[x_1, \dots, x_n]$. Suppose the system of $f_1, \dots, f_m \in k[x_1, \dots, x_m]$ has no solutions, i.e. there exist $u_1, \dots, u_m$ such that $1 = f_1u_1 + \dots f_mu_m$. Indeed, suppose not, then there exists a maximal ideal $M$ with $(f_1, \dots, f_m) \subset M$, then there exist $a_i$ such that $x_i - a_i$ generate it. So $(f_1, \dots, f_m) \subset (x_1-a_1, \dots, x_n - a_n)$, hence $(a_1, \dots, a_m)$ is a solution to this system, contradiction. \par 
    Now show strong Nullstellensatz. Suppose that $g \in k[x_1, \dots, x_n]$ vanishes at every point $p$ that's a solution to $\{f_i(p) = 0\}$ (this system has solutions). Then there exists $N$ such that $g^N \in (f_1, \dots, f_m)$. Consider $k[x_1, \dots, x_n] \subset k[x_1, \dots, x_n, t]$. Consider an ideal $J = (f_1(x_1, \dots, x_n), \dots, f_m(x_1, \dots, x_m), tg(x_1, \dots, x_m) - 1)$, we know $V(J) = \emptyset$. Then there exist some polynomials $u_1, \dots, u_{m+1} \in k[x_1, \dots, x_m, t]$ such that $u_1f_1 + \dots + u_mf_m + u_{m+1}(tg-1) = 1$. Plug in $t = \frac{1}{g}$, then $u_1(x_1, \dots, x_n, 1/g)f_1(x_1, \dots, x_n) + \dots = 1$, and 
    $$\frac{\tilde{u}(x_1, \dots, x_n)}{g^m}f_1(x_1, \dots, x_m) + \dots + \frac{\tilde{u}(x_1, \dots, x_m)}{g^m} = 1$$
    Multiply by $g^m$ on both sides and we are done. This type of reduction of strong to weak Nullstellensatz is called the Rabinovich trick. \par 
    Now we should probably prove the commutative algebra result we've been relying on. 
    \begin{lemma}
        If $A - B$ and $B - C$ are finite then $A - C$ is finite. 
    \end{lemma}
    \begin{proof}
        Express $A = a_1B + a_2B + \dots a_nB$, $B = b_1C + \dots b_mC$, plug in the latter into the former. 
    \end{proof}
    \begin{lemma}
        Suppose $A - B$ finite, then for $a \in A$ there exist $n \in \mathbb{N}$, $b_0, \dots, b_{n-1} \in B$ then $a^n + b_{n-1}a^{n-1} + \dots + b_0 = 0$ [$a$ is integral over $B$].
    \end{lemma}
    \begin{proof}
        $A$ is finite over $B$, thus there exist $a_1, \dots, a_n$ such that $A = Ba_1 + \dots + Ba_n$, so 
        $$\begin{cases}
            aa_1 = b_{11}a_1 + \dots + b_{1n}a_n \\
            aa_2 = b_{21}a_1 + \dots + b_{2n}a_n \\
            \vdots 
            aa_n = b_{n1}a_1 + \dots + b_{nn}a_n
        \end{cases}
        $$
        Subtracting $aa_i$ from the left we get a system, and from it we get 
        $$\det \begin{bmatrix}
            b_{11}-a & b_{12} & \dots & b_{1n} \\
            b_{21} & b_{22}-a & \dots & b_{2n} \\
            \vdots
            b_{n1} & b_{n2} & \dots & b_{nn}-a \\
        \end{bmatrix} = 0$$
        There exists an \textit{adjunct} matrix $X^*$ to the matrix $X$ above such that $X^*X = \det X \Id X*$. Then we also get 
        $(\det X)a_i = 0$ and $\det X = b_1a_1 + \dots $ (TODO) 
    \end{proof}
    Final lemma: $B[x] - B$ is finite.

\end{document} 