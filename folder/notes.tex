\documentclass[12pt]{article}
\usepackage[margin = 1 in]{geometry}
\usepackage{amsmath, amsfonts, graphicx, amsthm, quiver}

\newcommand{\F}{\mathbf{F}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\GL}{\mathrm{GL}}
\newcommand{\A}{\mathbb{A}}
\newcommand{\Aff}{\mathrm{Aff}}
\renewcommand{\vec}{\overrightarrow}
\newcommand{\PGL}{\mathrm{PGL}}
\newcommand{\Res}{\mathrm{Res}}
\newcommand{\Z}{\mathbb{Z}}

\newtheorem{fact}{Fact}
\newtheorem{definition}{Definition}
\newtheorem{example}[]{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}

\title{Algebraic Geometry Notes}
\author{Raman Aliakseyeu}
\date{Winter 2024}

\begin{document}
    \maketitle
    \noindent \textbf{Course by Daniil Rudenko.} \par 
    We will start with projective geometry, and then add algebra to it. No schemes in this course, we will be closer to the 19th century stuff. We are not following any particular book(s). Number one book is "Algebraic Geometry" by Shafarevich, our goal is all of chapter 1. We will start with projective geometry, a book for that is Prasolov (roughly). 
    \section{Projective Geometry} 
    \subsection{Lecture 1} 
        Algebraic geometry studies solutions to systems of algebraic equations/algebraic curves (which are loci of solutions of algebraic equations). One of the most famous renaissance geometry results is the Pascal theorem: the points $G, H, I$ in the diagram below are colinear for any configuration of $A, B, C, D, E, F$ on the circle. 
        \begin{center}
            \includegraphics[width = 0.5 \linewidth]{pascals-theorem.PNG}
        \end{center}
        A similar result to that is Pappus' theorem. Another top theorem is by Cayley-Salman 1849: A smooth projective cubic surface contains exactly 27 lines. Another such theorem is that any smooth projective curve of degree 4 has 28 bitangent lines (tangent in exactly two points). Working toward proofs of these results using algebraic geometry is our goal. \par 
        \textit{History of the subject}: throughout the 19th century it developed very quickly, people who were into Euclidean geometry just transitioned to algebraic geometry and discover miracles like we described above. During the late 19th century Italians, who before led algebraic geometry, started producing very incorrect results because the technical aspects became too hard. Oscar Zariski and others saved the subject by rigorizing it using commutative algebra, with the language of ideals and rings, etc. We will see some `wrong' arguments by the Italian school and see why they are not precise enough. There was another revolution by Grothendieck and the Bourbaki group in developing the language of schemes, which we will not touch in this course (``You will need to sacrifice a year of your life to start speaking that language. I never needed to, but a lot of people do.'')\par 
        (Rudenko plug: woollymathematics.com)\par 
        Take $\F$ to be some field (usually $\C$), and polynomials $p_1, \dots, p_k \in \F[x_1, \dots, x_n]$, and study the solutions to the system 
        $$\begin{cases}
            p_1(x_1, \dots, x_n) = 0 \\
            \vdots \\
            p_k(x_1, \dots, x_n) = 0
        \end{cases}$$
        \begin{fact}
            If $k = 1$ and $p_1(x) = x^d + a_{d-1}x^{d-1} + \dots + a_0$ for $d \geq 1$, $p_1(x) = 0$ has $\leq d$ solutions in $\F$. 
        \end{fact}
        If $k = 2$, the number of solutions is $\leq$ than the product of the degrees of the polynomials. Of course we usually want a precise number of solutions, but generally we can't hope for anything more than upper bounds. 
        \begin{fact}
            If $\F = \C$, then $p(x) = 0$ has a solution. 
        \end{fact}
        This is a remarkably hard result called the Fundamental Theorem of Algebra. There is a result that is in a way easier:
        \begin{fact}
            If $\F = \C$, then $p(x) = 0$ has exactly $d$ solutions if counted with multiplicity. 
        \end{fact} 
        We would hope for something like that but for $k > 1$. Bezout's theorem kind of satisfies that, but the picture is more complicated. For example, take a line and a hyperbola. While we would expect $4$ solutions up to multiplicity, but take a line intersecting one of the components of the hyperbola transversally that is parallel to one of the other component's asymptotes. However, everything becomes nicer if we change the complex plane to the projective plane. The study of geometry on the projective plane is the projective geometry, and that's what we will study first. \par 
        \begin{definition} \label{def:proj_plane}
            Let $\F$ be a field. A \textbf{projective space} $\P_\F^n$ is the set of lines in the vector space $\F^{n+1}$. 
        \end{definition}
        An example for $\F = \R$, $\P_{\R}^2$ is the set of lines through the origin in the real plane. We can think of $\P_\R^1$ as all the points of a line in $\R^2$ not through the origin (say the line $y = 1$ in $\R^2$) but with a point at infinity added. \par 
        In $\R^2$, usually two lines intersect at one point, and two points are contained in one line. We can also define $\P_\R^2$ as the set of points of $\R^2$, and the set $[l]$ (points at $\infty$), which are equivalence classes of lines in $\R^2$ by $l_1 \sim l_2$ for $l_1$ and $l_2$ are parallel. Note that there is one more line, line at infinity, consisting only of points $[l]$. \par 
        \begin{fact}
            In $\P^2_\R$ every two distinct lines intersect in one point and every two points are contained in precisely one line. 
        \end{fact}
        \begin{proof}
            Non-parallel lines intersect in the usual way. Parallel lines intersect at their equivalence class $[l]$. If one of the lines is the `line at infinity', they intersect at the equivalence class of the other line. Similarly we can examine the second part of the statement. 
        \end{proof}
        \begin{fact}
            For $\P_\R^2$, the abstract definition \ref{def:proj_plane} is equivalent to the definition we gave in the example. 
        \end{fact}
        \begin{proof}
            Take a plane in $\R^3$ not passing through the origin, $z = -1$ for example. Then we can create a bijection between the set of lines through the origin and the set of points on the plane and the set of points at infinity, by mapping the lines that intersect the plane with their intersection point, and those that are parallel with the plane to their equivalence class on the plane $\R^2$. Thus we have a bijection between the two objects we defined. 
            \begin{center}
                \includegraphics[width = 0.4\linewidth]{proj-plane.jpg}
            \end{center}
        \end{proof}
        Another way to think of $\P_\R^2$ is as a sphere with antipodal points identified. \par
        ``ChatGPT can make you Snake 4: Snake but on the projective plane.''
        
    \subsection{Lecture 2} 
    A \textbf{geometry} is a set $X$ along with a group $G$ acting on $X$, $x \mapsto gx$, which defines `equivalences' between subsets of $X$. 
    \begin{example}
        \begin{itemize}
            \item Let $X = \R^2$, $G$ the group of \textbf{isometries}: distance preserving bijections. Indeed, isometries also preserve lengths, areas, angles, etc. Think of them as congruences in Euclidean geometry. 
            \item Let $X$ be a vector space, $G = \text{GL}(V)$, the general linear group. More specifically, let $X = F^n$ for some field $G$, then $G = \GL_n(F)$.  
        \end{itemize}
    \end{example}
    \begin{definition}
        Fix $F$ some field. Define \textbf{affine space} $\A_F^n = F^n$, with the group of \textbf{affine transformations} $$\mathrm{Aff}_n = \{f: \A^n \to \A^n \mid f(x) = Ax + b, A \in \GL_n(F), b \in A^n\}$$
    \end{definition}
    A typical way of thinking about affine space is as a vector space without a fixed origin. 
    \begin{example}
        Circles don't make much sense in $\A_\R^2$ anymore, since $x^2 + y^2 = 1$ is equivalent to an ellipse (say by $(x, y) \mapsto (2x, 5y)$). 
    \end{example}
    \begin{definition}
        An \textbf{affine algebraic variety} is a set of solutions of a system of polynomials $P_1, \dots, P_k \in F[x_1, \dots, x_n]$:
        $$\begin{cases}
            P_1(x_1, \dots, x_n) = 0 \\
            \vdots \\
            P_k(x_1, \dots, x_n) = 0
        \end{cases}$$
    \end{definition}
    \begin{example}
        \begin{itemize}
            \item $P_1(x, y) = 2x + y - 1 = 0$ gives a line; $P_1(x, y) = x^2 + y^2 - 1 = 0$ gives a circle.
            \item $P_1(x, y, z) = 2x+y-z-1 = 0$ and $P_2(x, y, z) = x + y -3 = 0$, then the variety is an intersection of two planes, thus a line.  
        \end{itemize}
    \end{example}
    If $P_1, \dots, P_k$ are linear $\sum a_i x_i + b$ then we will call it \textbf{affine subspace}.\par 
    We can identify pairs of points in $\A^n$ with vectors in a vector space $F^n$, where vectors $\overrightarrow{AB}$ are pairs of points $(A, B)$, identified via the equivalence relation $\overrightarrow{A_1B_1} \sim \overrightarrow{A_2B_2}$ iff $(A_1)_i - (B_1)_i = (A_2)_i - (B_2)_i$. An observation is that if $f: \A^n \to \A^n$ is an affine transformation, then $f(\overrightarrow{AB}) = \overrightarrow{f(A)f(B)}$. \par 
    What we just described is a homomorphism $\mathrm{Aff}_n \to \GL_n$ which maps $f = Ax + b$ to $A \in \GL_n$. The kernel of this map is isomorphic to $\F^n$ as an additive group. \par 
    \begin{definition}
        If $A_1, \dots, A_{n+1} \in \A^n$ are in \textbf{general position} if $\overrightarrow{A_1A_i}$ for $i = 2, 3, \dots n+1$ are all linearly independent. Equivalently (exercise) $A_1, \dots, A_{n+1}$ are not in the same affine hyperplane. 
    \end{definition}
    \begin{theorem}
        Suppose $A_1, \dots, A_{n+1} \in \A^n$ and $B_1, \dots, B_n \in \A^n$ are points in general position in $\A^n$. Then there is a unique $f \in \Aff_n$ s.t. $f(A_i) = B_i$ for each $i$.  
    \end{theorem}
    \begin{proof}
        [Proof sketch:] Using translation by a vector $\overrightarrow{A_1B_1}$, move $A_1$ to $B_1$. This reduces this problem to the uniqueness of a linear transformation from the basis $\overrightarrow{A_1A_i} \to \overrightarrow{B_1B_i}$. 
    \end{proof}
    Observation: ``$X$ is the midpoint of the segment $\overline{A_1A_2}$'' is an affine property: $f(X)$ is still the midpoint of $\overline{f(A_1)f(A_2)}$. Application of this: 
    \begin{theorem}
        Three medians of a triangle intersect at a point. 
    \end{theorem} 
    \begin{proof}
        Let $ABC$ be an arbitrary triangle in $\A_\R^2$. Let $f$ be the affine transformation taking an equilateral triangle to $ABC$. The medians of an equilateral triangle intersect at a point by symmetry. But medians get sent to medians, and intersections are preserved, so the medians of $ABC$ must intersect also. 
    \end{proof}
    \begin{definition}
        Let $V$ be a vector space over $F$, then its \textbf{projectivization} $\P(V)$ is the set of lines (1-dim vector subspaces) in $V$. 
    \end{definition}
    By definition the dimension of $\P(V)$ is $\dim V - 1$. If $V = F^{n+1}$, $\P(V) = \P_F^n$. More explicitly, $\P(V) = (V - \{0\})/(v \sim \lambda v, \lambda \in F)$. 
    We express points in $\P(V)$ by \textbf{homogeneous coordinates} so $[X_0: X_1 : \dots : X_n]$, with the aforemetioned equivalence relation (so e.g. $[0, 1, 0] = [0, 2, 0]$) \par 
    ``Gold to silver to whiskey, the ratio should be 1 to 1 to 100. You're making a cocktail. Don't do that by the way... Point of projective geometry is to make cocktails.''\par 
    If $A = [X_0: \dots :X_n] \in \P^n$. If $X_n \neq 0$ then $A = [X_0/X_n: \dots : X_{n-1}/X_n, 1]$, points of this form this are in bijection with $\A^n$. Indeed, $\P^n = \A^n \sqcup \A^{n-1} \sqcup \dots \sqcup \A^0$. The points with $X_n = 0$ are in bijection with points in $\P^n$ (divide all entrees by the first non-zero entry from the right). For example, $\P_\R^1 = \R \cup \infty$, where $\R$ is in bijection with points $[X_0/X_1:1]$ and $\infty$ represents $[1:0]$.  \par 
    \textbf{Barycentric coordinates} are basically the same thing as homogeneous coordinates but in a different language. Imagine putting masses $m_1, m_2, m_3$ at the vertices of a triangle, then look at their center of mass. If $m_1 = m_2 = m_3$ then this point is the centroid. More precisely, if $O$ is any point, we have that the point $M$ with homogeneous coordinates $(m_1, m_2, m_3)$, the center of mass for some $m_1, m_2, m_3$ (sum non-zero), is such that
    $$\vec{OM} = \frac{m_1\vec{OA_1} + m_2\vec{OA_2} + m_3\vec{OA_3}}{m_1 + m_2 + m_3}$$
    As such, $O$ has homogeneous coordinates $(m_1:m_2:m_3)$ in the projective plane.  \par 
    A homogeneous polynomial is one where each term has the same degree. Then it makes sense to talk about $P([x_0, \dots, x_d]) = 0$ for a homogeneous polynomial $P$, since such polynomials respect the equivalence relation of homogeneous coordinates. 
    \begin{definition}
        \textbf{Projective variety} is a subset of $\P_F^n$ which is the set of solutions of a system:
        $$\begin{cases}
            P_1([X_0: \dots : X_n]) = 0 \\ 
            \vdots \\
            P_k([X_0: \dots : X_n]) = 0
        \end{cases}$$
        where each $P_i$ is homogeneous. 
    \end{definition}
    \begin{example}
        Say $P(x, y, z) y - x^2$, which we can think of as points $[x:y:1]$ that satisfy the parabola. We can relabel $[x:y:1]$ into $[X:Y:Z]$ with $x = X/Z, y = Y/Z$, so the set of points satisfying $P_1([X:Y:Z])$ is the set such that $X^2 = YZ$. So for instance $[0:1:0]$ is also a solution. We can think of this as the parabola becoming an ellipse because it closes at infinity. 
    \end{example}

    \subsection{Lecture 3}
    \textbf{Observation}: If we have an injective linear map $f: V_1 \to V_2$, and $L$ is a line in $V_1$, then $f(L)$ is also a line. So, $f$ induces a map $\overline{f}: \P(V_1) \to \P(V_2)$.
    \begin{definition}
        If $V_1 = V_2$ in the above, then the set of maps $\P(V) \to \P(V)$ induced by linear maps $V \to V$ form the \textbf{projective linear group} $\PGL(V)$. If $V = \F^n$, we denote $\PGL(V) = \PGL_n(\F)$. 
    \end{definition}
    $\PGL_n(F)$ has a simple description: Let $\Phi: \GL_n(\F) \to \PGL_n(\F)$ be defined by $f \to \overline{f}$. Then $\ker \Phi$ is the set of all diagonal matrices (since those are identity on one-dim spaces), which is isomorphic to $F^\times$. So, $\PGL_n(\F) = \GL_n/F^\times$ by first isomorphism theorem. \par
    Now consider $\P_\F^1$, and $\begin{bmatrix}
        a & b \\ c & d
    \end{bmatrix} \in \GL_n(\F)$. Then $\overline{f}([x: y]) = [ax+by: cx+dy]$. Note that we can interpret $\P_\F^1 = \A_\F^1 \cup \infty$, where $A_\F^1$ can be thought of as the set of all $[x:1]$, and $\infty$ can be thought of as $[0:1]$. So on $\A_\F^1$, $[ax+by: cx+dy] = [\frac{ax+b}{cx+d}:1]$. So on the affine line, $\overline{f}$ sends $x$ to $\frac{ax+b}{cx+d}$. \par 
    If $f(x) = 1/x$, then $[x:y] = [x/y:1]$ gets sent to $[y/x:1] = [y:x]$ by $\overline{f}$. Hence, $[0:1] \mapsto [1:0]$, or in other words $0 \mapsto \infty$ in $\P_\F^1$. This corresponds to the analytic intuition: as $x$ gets closer to $0$, $1/x$ gets closer to infinity. \par 
    Now look at $\P_\F^2$, and consider the linear transformation 
    $$A = \begin{bmatrix}
        a_{11} & a_{12} & a_{13} \\
        a_{21} & a_{22} & a_{23} \\
        a_{31} & a_{32} & a_{33}
    \end{bmatrix}$$
    $\overline{A}$ sends $[x_1: x_2: x_3]$ to $[a_{11}x_1 + a_{12}x_2 + a_{13}x_3: \dots]$. So, as a map on $\A_F^2$, 
    $$\overline{A}(u, v) =  \left( \frac{a_{11}x_1 + a_{12}x_2 + a_{13}x_3}{a_{31}x_1 + a_{32}x_2 + a_{33}x_3}, \frac{a_{11}x_1 + a_{12}x_2 + a_{13}x_3}{a_{31}x_1 + a_{32}x_2 + a_{33}x_3}\right)$$
    ``If you have a dictator you don't like and you want to get rid of them, you can just take a projective transformation of them and send them to infinity. That's what they used to do with dictators.'' \par
    By this he means, by choosing the last row of $A$ appropriately, we can get $\overline{A}$ to send any given line in $\A_\F^2$ to infinity. 
    \begin{theorem}
        Consider points $A_1, \dots A_{n+2}$, $B_1, \dots, B_{n+2}$ in $\P^n$ such that $\{A_i\}$ and $\{B_i\}$ are in \textit{general position} (on $\P^n$ this means for any $i$, $A_1, \dots, \hat{A_i}, \dots, A_{n+2}$ span $F^{n+1}$ when considered as vectors in $F^{n+1}$). Then there exists $\overline{f} \in \PGL_n(\F)$ such that $\overline{f}(A_j) = B_j$ for $1 \leq j \leq n+2$. 
    \end{theorem}
    \begin{proof}
        Let $A_i = [v_i]$, $B_i = [w_i]$, where $[v_i]$ is the equivalence class of some vector $v_i \in \F^{n+1}$. Then by assumption $v_1, \dots, v_{n+1}$ and $w_1, \dots, w_{n+1}$ are bases of $F^{n+1}$, so there exists a unique linear transformation $f_1$ such that $f_1(v_i) = w_i$. Then, $f_1(v_{n+2}) = \tilde{v}_{n+2}$. Since $v_{n+2} = \lambda_1v_1 + \dots + \lambda_{n+1}v_{n+1}$, this maps to $\tilde{v}_{n+2} = \lambda_1w_1 + \dots \lambda_{n+1}w_{n+1}$. Also $w_{n+2} = \mu_1w_1 + \dots + \mu_{n+1}w_{n+1}$. Note that this means $\lambda_i, \mu_i \neq 0$ for any $i$. Then, let $f_2$ be a linear map sending $w_i$ to $\frac{\mu_i}{\lambda_i}w_i$ so $f_2 \circ f_1(v_1) = \frac{\mu_1}{\lambda_i}w_i$ and $f_2 \circ f_1(v_{n+2}) = w_{n+2}$. As such, $\overline{f_2f_1}$ sends $A_i$ to $B_i$. Uniqueness is left as an exercise. 
    \end{proof}
    \begin{theorem}
        [Pappus' Theorem] Consider the diagram 
        \begin{center}
            \includegraphics[width = 0.7\linewidth]{pappus.png}
        \end{center} 
        The points $X, Y, Z$ are always colinear. [Note: the diagram he drew in class had points labelled differently, I changed them because this diagram is from the internet. ]
    \end{theorem}
    \begin{proof}
        Take the line through $Y$ and $Z$, send it via a projective transformation to the line at infinity. Then, to show $X$ is on the same line as $Y, Z$ it suffices to show that after the transformation $\overline{AB'}$ is parallel to $\overline{A'B}$ (marked red in the diagram below:)
        \begin{center}
            \includegraphics[width = 0.8\linewidth]{pappus_step_2.png}
        \end{center} 
        This is true by angle chasing using the parallel line postulate.  
    \end{proof}

    For $\P^1$, recall that $\PGL_2(\F) = \{\frac{ax+b}{cx+d} \mid ad - bc \neq 0\}$. By the theorem above, elements of $\PGL_2(\F)$ send any three distinct points to any three distinct points. This is not true however for four points, so it makes sense that there would be some sort of invariant associated with four points. 
    \begin{definition}
        If $x_1, x_2, x_3, x_4 \in \P^1$ are distinct, \textbf{cross ratio} of these points is defined 
        $$[x_1, x_2, x_3, x_4] = \frac{(x_1-x_2)(x_3-x_4)}{(x_1-x_4)(x_3-x_2)}$$
        If $x_4 = \infty$, 
        $$[x_1, x_2, x_3, \infty] = \frac{x_1 - x_2}{x_3 - x_2}$$
        Similarly define for any other $x_i = \infty$. 
    \end{definition}
    \begin{theorem}
        If $f \in \PGL_2(\F)$ then $[f(x_1), f(x_2), f(x_3), f(x_4)] = [x_1, x_2, x_3, x_4]$. So, cross ratio is an invariant of $\PGL_2(\F)$.
    \end{theorem}
    \begin{proof}
        $\PGL_2(\F)$ as a group is generated by the following transformations: $x \mapsto x + b$, $x \mapsto ax$, $x \mapsto 1/x$. It is easy to see that the first two leave cross ratio unchanged. The third is an algebra bash. 
    \end{proof}
    Note that $[x_1, x_2, x_3, x_4] = [x_{\sigma(1)}, x_{\sigma(2)}, x_{\sigma(3)}, x_{\sigma(4)}]$ for $\sigma \in K$, the Klein $4$-group in $S_4$, so in fact $\sigma \in S_4$ can only change $[x_1, x_2, x_3, x_4]$ into $\lambda, 1-\lambda, 1/\lambda, 1-1/\lambda, \lambda/(\lambda-1), 1/(1-\lambda)$. Proving this will be an exercise. \par 
    \begin{proposition}
        Let $l_1, l_2 \in \P^2$, $O \in \P^2$ not on $l_1$ or $l_2$. Consider a projection of $l_1$ onto $l_2$ about $O$, as in the diagram (with $O$ unlabelled on top): 
        \begin{center}
            \includegraphics[width = 0.4\linewidth]{crossratio.png}
        \end{center}
        More precisely, if $A' \in l_1$ and $\vec{OA'}$ is a line in $\P^2$ containing both $O$ and $A'$, this map is $A' \mapsto A = \vec{OA'} \cap l_2$. This is a projective transformation. 
    \end{proposition} 
    Note that using this diagram, the invariance of cross ratio can be interpreted as the equivalence of ratios segments, as such: 
    $$\frac{|C'A'|}{|C'B'|}: \frac{|D'A'|}{|D'B'|} = \frac{|CA|}{|CB|}: \frac{|DA|}{|DB|}$$
    [A proof of this equality with elementary trigonometry is in Prasolov]
    \begin{proof}
        Let $L_1, L_2$ be planes and $O$ be a line in $V = \F^3$ not contained in either plane. Take the plane $V/O$, then $\P(V/O) = \P^1$. Then $L_i \to V \to V/O$, the composition of inclusion and quotient, gives us a map $\P^1 \to \P^1$. Indeed, we have the commutative diagram
        \[\begin{tikzcd}
            & V && V \\
            {L_1=\mathbb{P}^1} && {V/O=\mathbb{P}^1} && {L_2 = \mathbb{P}^1}
            \arrow[hook, from=2-1, to=1-2]
            \arrow[hook, from=2-5, to=1-4]
            \arrow[from=1-2, to=2-3]
            \arrow[from=1-4, to=2-3]
            \arrow[from=2-1, to=2-3]
            \arrow[from=2-5, to=2-3]
        \end{tikzcd}\]
        With the bottom two arrows being isomorphisms. Composing the left arrow with the inverse of the right arrow gives us a map in $\GL_2(F)$ between $L_1$ and $L_2$, which induces the projection map in $\PGL_2(F)$.  
    \end{proof}

    \subsection{Lecture 4}
    (OH 5PM) \par 
    [TODO: there are some errors in this lecture] \par
    Recall: We consider a field $\F$, mostly $\R, \C$ or $\F_p$. As we defined before, $\P_\F^n = (\F^{n+1} \setminus 0)/\F^\times$, where the identification is $(x_0, \dots, x_n) \sim (\lambda x_0, \dots, \lambda x_n)$. An equivalence class like this is denotes via homogeneous coordinates $[x_0: \dots : x_n]$. We can identify the affine plane $\A_\F^n$ by considering the set of points with $x_n \neq 0$ (i.e. by excluding the lines at infinity). \par 
    Note we may write $R = \F[x_0, \dots, x_n] = R_1 \oplus R_2 \oplus \dots$ where $R_d$ is a vector space of homogeneous polynomials with degree of terms being $d$ (so $x_0^{k_0}x_1^{k_1} \dots x_n^{k_n}$ with $\sum k_i = d$). $R_0$ is the base field. $R_1$ is the dual space $V^*$ of $\F^n$ (since all homog. polys of deg 1 are precisely all linear maps). $R_2 = \mathbb{S}^2 V^*$, and in general $R_d = \mathbb{S}^d V^*$. 
    \begin{definition}
        Let $F \in R_d$ be a homog. polynomial with $d \neq 0$. Then the set of solutions of an equation $F(x_0, \dots, x_n) = 0$ is called a \textbf{hypersurface} of degree $d$ in $\P^n$.  
    \end{definition}
    \begin{example}
        In $\P^3$, $x_0^3 - x_1^3 + x_2x_3^2 + x_3^3 = 0$. The intersection of this with $\A^3$ (obtained by dividing all terms by $x_3$) we get $(x_0/x_3)^3 - (x_1/x_3)^3 + x_2/x_3 + 1 = 0$. The rest of this hypersurface is in $\P^3 \setminus \A^3 = H$, it is defined by $x_0^3 - x_1^3 = 0$. Since $H \cong \P^2$, we can do the same procedure to reduce this hypersurface to something we can draw in $\A^2$, getting $y_0^3 - y_1^3 = 0$, which are three lines intersecting at the origin (if $\F = \C$). Going the other way from $y_0^3 + y_1^3 + y_2 + 1 = 0$ by introducing another variable and going into $\P^3$ is called \textit{homogenization}. 
    \end{example}
    \begin{definition}
        A \textbf{projective variety} is a \textit{finite} intersection of hypersurfaces. 
    \end{definition}
    (Note: you do not get more sets if you extend this to an infinite system, by the Hilbert basis theorem, to be covered later). \par 
    \begin{definition}
        A hypersurface $X = \{F = 0\}$ for $F \in R_d$ is \textbf{irreducible} precisely when $F$ is irreducible. 
    \end{definition}
    \begin{example}
        \begin{itemize}
            \item $x_0x_1 = 0$ in $\P^2$ is a union of two lines. Not irreducible polynomial, so the hypersurface is not irreducible also. In general, any union of two lines is not irreducible. 
            \item $x_0^2 + x_1^2 = x_2^2$ is irreducible, so its hypersurface, a circle in $\A^2$. 
        \end{itemize}
    \end{example}
    Assume $\F = \C$. Consider  the action of $\PGL_{n+1}$ on degree $d$ hypersurfaces in $\P^n$. Take $d = 1$, then this action is transitive, as we saw last time. Thus up to projective transformations all hyperplanes are the same. If $d = 2$, $F$ looks like 
    $$\sum a_{ij} x_i x_j = (x_0, \dots, x_n) A \begin{pmatrix}
        x_0 \\
        \vdots \\
        x_n
    \end{pmatrix} = x^\top A x$$
    Subject to a projective transformation, $x = Uy$ for some $y \in \P^n$. Thus, $x^\top A x \mapsto y^\top U^\top A Uy = 0$.  
    \begin{theorem}
        Any quadric is a projectively equivalent to a quadric $\lambda_0 x_0^2 + \lambda_1 x_1^2 + \dots + \lambda_n x_n^2 = 0$
    \end{theorem}
    [TODO: This is equivalent to (some) matrix diagonalization result, didn't catch which one that was.]
    Exercise: prove this my hand for $n = 2$. \par 
    So, if $\F = \R$, for $n = 2$ the picture is complicated, for example $x_0^2 - x_1^2 - x_2^2 = 0$ is a double cone. But for $\F = \C$ things become simpler because everything can be reduced to the cases $x_0^2 + x_1^2 + \dots + x_i^2 = 0$. So in $\P_\C^2$ we have lines, intersections of two lines, and conics, for $i = 1, 2, 3$ respectively. Thus the action of $\PGL_3$ has three orbits, namely those classes. \par 
    Now we discuss projective duality. Suppose we are working in $\P^2$. A line here is defined by an equation $ax + by + cz = 0$ with not all $a, b, c = 0$, modulo rescaling. Thus lines in $\P^2$ also form a projective space. This has some striking applications. \par
    Conceptually, if we have $\P^n = \P(V)$ for an $n$-dim vector space $V$, hyperplanes in $\P^n$ are in bijection with the elements of $\P(R_1) = \P(V^*)$. \par 
    The \textbf{duality principle} can be stated as follows. Consider any theorem about points and lines in $\P^2$, replacing lines by points and vice versa gives another valid theorem. 
    \begin{example}
        Recall Pappus' theorem. The dual to this theorem can be drawn as follows: (TODO: wrong diagram)
        \begin{center}
            \includegraphics[width = 0.7\linewidth]{pappus dual.png}
        \end{center}
        The statement is that lines $a$, $b$, and $c$ (passing through $C_i$) all pass through $O$. 
    \end{example} 
    In a dualization of a theorem, for every line there is a point, for every point there is a line. If $n$ lines intersect at a point, in the dual theorem $n$ points will lie on one line. \par 
    In $\P^3$ a similar duality holds: points correspond to planes, lines correspond to lines, planes correspond to points. \par 
    Three important facts about $V^*$: 1. $\dim V = \dim V^*$ since $V$ and $V^*$ are (not canonically) isomorphic, 2. $V \cong (V^*)^*$ canonically via an isomorphism $v \mapsto (\phi: V \to F \mapsto \phi(v))$, 3. There is an inclusion reversing bijection between subspaces of $V$ and $V^*$, which is equivalent to $\dim W + \dim W^\top = \dim V$. 

    \subsection{Lecture 5}
    Let's talk about conics a little bit more. If $\P_\F^n = \P(\F^{n+1})$, so $[x_0: x_1: \dots : x_n]$ are homogeneous coordinates on $\P_\F^n$, and $P(x_0, \dots, x_n)$ is a homogeneous polynomial of degree $d$, $\{[x_0, \dots, x_n] \mid P(x_0, \dots, x_{n+1}) = 0\} \subset \P^n$ is a hypersurface of degree $d$. For $d = 1$ this is the familiar hyperplane. If $d = 2$ this is a \textit{quadric}. \par 
    Over $\C$ (polynomials with coeffs in $\C$), every quadric is either a union of two lines, a cross of two lines, or a conic (which are all the same on the projective plane). The conic is called the \textit{smooth} quadric. \par 
    \begin{theorem}[Pascal]
        \hfill
        \begin{center}
            \includegraphics[width = 0.4\linewidth]{pascal.png}
        \end{center}
        For any ellipse, putting distinct $A, B, C, D, E, F$ on it and connecting them a la Pappus' theorem gives us colinear intersection points $X$, $Y$, $Z$. 
    \end{theorem}
    ``Who thinks this is beautiful? If you don't [points to door]''\par 
    In this way Pappus' theorem is not surprising because both a pair of lines and a conic are quadrics over $\C$. \par 
    \begin{lemma}[Chasles]
        Let points $A_1, A_2, A_3, A_4$ and $X$, $Y$ lie on a conic. Then 
        $$[(XA_1), (XA_2), (XA_3), (XA_4)] =  [(YA_1), (YA_2), (YA_3), (YA_4)]$$
    \end{lemma}
    Notice that there is a bijection between points of a conic and lines through some particular point $X$ on a conic $C$. Indeed, a line through $X$ is either tangent to the conic or intersects it at some other point. Associating the tangent line to $X$ and the other lines to their intersection points, we associate the points of $C$ with $\P^1$. Indeed, the result above follows by using the two bijections onto $\P_1$ given by $X$ and $Y$, and concluding that there is a projective transformation mapping the $X$ configuration to the $Y$ configuration. This however requires some details we haven't gone over, so below is an elementary proof:
    \begin{proof}
        (TODO: diagram) First, assume the conic $C$ is a circle in $\R^2 \subset \P_\R^2$. If the points are ordered $A_1, \dots, A_4$ from left to right, let $U$ and $V$ be intersection points of a line $A_1A_4$ through $XA_2, XA_3$. Then
        $$[XA_1, \dots, XA_4] := [X_1, U, V, X_4] = \pm \frac{S_{A_1XY} S_{VXA_1}}{S_{A_1XA_2}S_{UXV}} = \frac{\sin \angle X_1XU \sin \angle VXA_1}{\sin \angle A_1XA_2 \sin \angle UXV}$$
        where $S_{ABC}$ is the notation for the area of triangle $ABC$. But by [these angles from $X$ and $Y$ are called something I forget what], we know that they remain invariant if we replace $X$ with $Y$. We thus have shown that the cross ratio remains the same. 
    \end{proof}
    \begin{proof}
        [Proof: (Of Pascal's)] By the lemma above, 
        $$[A_1B_1, A_1B_2, A_1B_3, A_1A_2] = [A_3B_1, A_3B_2, A_3B_3, A_3A_2]$$
        Now label the two intersection points above the line $XYZ$ in the diagram $S$ and $T$. Then by definition of cross ratio of four lines, 
        $$[B, Z, S, A_2] = [T, X, B_3, A_2]$$
        by picking the transversal line to be $A_2B_1$ and $A_2B_3$ for the left and right cross ratios respectively. But this quantity is also equal to $[(B_1Y), (ZY), (SY), (A_2Y)]$. We want $X$ and $Z$ to be colinear with $Y$, let $X'$ be the intersection of the line $ZY$ with $A_2B_3$. Then, 
        $$[(B_1Y), (ZY), (SY), (A_2Y)] = [T, X', B_3, A_2] = [T, X, B_3, A_2]$$
        This implies $X = X'$, which concludes the proof. 
    \end{proof}
    Our goal now is to go towards Bezout's theorem. The statement of the theorem is as follows: In $\P_\C^2$ let $X$ and $Y$ be projective curves such that they do not have common irreducible components. Then the number of points in their intersection is less than or equal to the product of their degrees, and in fact (hard part) is \textit{equal} up to multiplicity. Of course, it's hard because it's hard to define exactly what `up to multiplicity' means. Take two concentric circles $x^2 + y^2 = 1$ and $x^2 + y^2 = 2$ in $\A^2$. These only have solutions once you homogenize them and pass over to the complex projective plane. Then, $x^2 + y^2 = z^2, x^2 + y^2 + 2z^2$ have solutions $[1:i:0], [1:-i:0]$. By Bezout's these points have to have intersection multiplicty $2$, so in fact they are \textit{tangent points}. \par 
    For now, we will only prove the easier upper bound on the number of intersection points. 
    \begin{theorem}[Sylvester]
        Let $A(x) = a_mx^m + a_{m-1}x^{m-1} + \dots a_0 = 0$ and $B(x)$ be polynomials of degree $m$ and $n$ respectively. They have a common factor of degree $\geq 1$ if and only if the following $(m+n) \times (m+n)$ matrix
        $$\begin{bmatrix}
            a_m & a_{m-1} & \dots & a_0 & 0 & \dots & 0 \\
            0 & a_m & \dots & a_1 & a_0 & \dots & 0 \\
            & & \vdots & & \\
            0 & 0 & \dots & a_m & \dots & & a_0 \\
            b_n & b_{n-1} & \dots & &  b_0 & \dots & 0 \\
            0 & b_n & \dots & & & \dots & 0 \\
            & & \vdots & & \\
            0 & 0 & \dots b_n & & \dots & & b_0 \\
        \end{bmatrix}$$
        (where $a_i$ appear in the first $n$ rows, $b_j$ appear in the last $m$ rows) has determinant $0$. The determinant is called the \textbf{resultant} of $A$ and $B$. 
    \end{theorem}
    \begin{proof}
        $A$ and $B$ have a common factor iff there exist non-zero polynomials $U$ and $V$ of degrees less than $n$ and $m$ respectively, such that $A\cdot U = B \cdot V$. This is a linear combination in the first $n$ rows being equal to a linear combination of the last $m$ rows, hence this condition holds iff $\det = 0$.  
    \end{proof}
    You can use this theorem to solve systems of polynomial equations by eliminating a variable, writing the condition that there is a solution in terms of the resultant. This is highly impractical if the degrees of equations are large, but is a useful proof technique. 

    \subsection{Lecture 6}
    Note that to generalize Sylvester's theorem to also work for polynomials over a ring and not a field we need to use Gauss' lemma. 
    \begin{theorem}
        Suppose that $f(x) = a_m(x-t_1)\dots (x-t_m)$, and $g(x) = b_n(x-s_1)\dots (x-s_n)$. Then their resultant is
        $$\Res(f, g) = a_m^n b_n^m \prod_{i=1}^m \prod_{j=1}^n (t_i - s_j)$$
    \end{theorem}
    \begin{proof}
        Let's view $\Res(f, g)$ as a polynomial in $\Z[a_m, b_n, t_1, \dots, t_m, s_1, \dots, s_n]$ (can do by Vieta's formulas). Looking at the resultant matrix, we can extract an $a_m$ and $b_n$ from every row of the determinant, we get $\Res = a_m^n b_n^m P(t_1, \dots, t_m, s_1, \dots, s_n)$. \par 
        Now we mention a little trick: for $f(x)$ with coefficients in a unique factorization domain, $f(x_0) = 0$ iff $(x-x_0)$ divides $f(x)$. Indeed, take $\tilde{f}(x) = f(x + x_0) = \tilde{a}_nx^n + \dots + \tilde{a}_0$. But then $f(x) = \tilde{a}_n(x-x_0)^n + \dots + \tilde{a}_0$, which $(x - x_0)$ divides if and only if $\tilde{a}_0 = 0$, i.e. $x_0$ is a root. \par 
        Thus, $P$ is divisible by $(t_i - s_j)$, since considering $P$ as a polynomial with coefficeints in $\Z[t_1, \dots, \hat{t}_i, \dots, t_m, s_1, \dots]$. Thus, $P(s_j) = 0$ implies $(t_i - s_j) \mid P$. Note that $(t_i - s_j)$ are pairwise coprime, as such 
        $$\Res(f, g) = Ca_m^n b_n^m \prod \prod (t_i - s_j)$$
        Note that the product $\prod \prod (t_i - s_j)$ has the same degree $nm$ as $\Res(f, g)$ (exercise), so by plugging in a point we realize $C = 1$. 
    \end{proof}
    (Discriminants, Resultants, and Multidimensional Determinants - very underrated book)\par 
    \begin{theorem}
        [Weak Bezout] Let $F$ be an infinite field, consider two curves with no common irreducible components in $\P_\F^2$, defined by $P(X, Y, Z) = 0$, $Q(X, Y, Z) = 0$ with $P$ and $Q$ homogeneous (no common irreducible components means $P$ and $Q$ are coprime). Then, $P$ and $Q$ have at most $\deg P \deg Q$ solutions. 
    \end{theorem}
    \begin{proof}
        Let $\deg P = m$, $\deg Q = n$. Assume the system $\{P = 0, Q = 0\}$ has $nm + 1$ solutions of the form $(X_i, Y_i, Z_i)$. Note there exists a coordinate system such that \textbf{1}. $P$ and $Q$ are not divisible by $Z$ and \textbf{2}. $Z_i \neq 0$ \textbf{3}. $\frac{Y_1}{Z_1}, \dots, \frac{Y_{n+m}}{Z_{n+m}}$. Then we have $f(x, y) = \frac{P(X, Y, Z)}{Z^m}$, $g(x, y) = \frac{Q(X, Y, Z)}{Z^n}$ are polynomials of degree $m$ and $n$ respectively in $X/Z$, $Y/Z$. They also have $nm + 1$ solutions $(x_i, y_i)$ with all $y_i$ distinct, and $f$ and $g$ have no common factor of degree $\geq 1$. Let's view $f(x, y), g(x, y)$ as polynomials in $(\F[y])[x]$. Note that $\Res(f, g)$, a polynomial in $\F[y]$, is nonzero with roots $y_i, \dots, y_{nm+1}$. It remains to show that $\deg \Res(f, g) < nm$. Left as a (non-trivial) exercise. 
    \end{proof}
    An application of this theorem. Suppose we are working in $\P_\C^2 = \P(\C^3)$. Curves of degree $1$ are lines, they form $\P(V^*) = \P^2$ as we have seen before. Curves of degree $2$ of form $ax^2 + bxy + cy^2 + dxz + eyz + fz^2$ form a space $\P^5$ (one less than the coordinates). All degree $2$ curves containing a point $[x_0, y_0, z_0]$ form a ($4$-dimensional) hyperplane in $\P^5$. Space of degree $2$ curves containing $2$ points $P_1, P_2$ will have dimension at least $3$, but it can be $4$. Degree $2$ curves containing $P_1, P_2, P_3$ will have dimension at least $2$, ... we thus can show that any $5$ points in $\P^2$ lie on a curve of degree $2$. 
\end{document} 